{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Target Encoding",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "F3spAaSlLozj",
        "g__EEWRPLuEe",
        "pJR4JtnALzk7",
        "d_CvINwf2v9f",
        "WAyX99su6Ztp",
        "_zSd5DXn16YA",
        "hNryKyVCM0cr",
        "zroBmNwL08_u",
        "pXAZQCjmNiYz",
        "GgfpH7-bYWFn"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "wlMOSchK2noH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Target Encoding\n",
        "\n",
        "Most machine learning algorithms require the input data to be a numeric matrix, where each row is a sample and each column is a feature.  This makes sense for continuous features, where a larger number obviously corresponds to a larger value (features such as voltage, purchase amount, or number of clicks).  How to represent categorical features is less obvious.  Categorical features (such as state, merchant ID, domain name, or phone number) don't have an intrinsic ordering, and so most of the time we can't just represent them with random numbers.  Who's to say that Colorado is \"greater than\" Minnesota?  Or DHL \"less than\" FedEx?  To represent categorical data, we need to find a way to encode the categories numerically.\n",
        "\n",
        "There are quite a few ways to encode categorical data.  We can simply assign each category an integer randomly (called label encoding).  Alternatively, we can create a new feature for each possible category, and set the feature to be 1 for each sample having that category, and otherwise set it to be 0 (called one-hot encoding).  If we're using neural networks, we could let our network learn the embeddings of categories in a high-dimensional space (called entity embedding, or in neural NLP models often just \"embedding\").\n",
        "\n",
        "However, these methods all have drawbacks.  Label encoding doesn't work well at all with non-ordinal categorical features.  One-hot encoding leads to a humongous number of added features when your data contains a large number of categories.  Entity embedding can only be used with neural network models (or at least with models which are trained using stochastic gradient descent).\n",
        "\n",
        "A different encoding method which we'll try in this post is called target encoding (also known as \"mean encoding\", and really should probably be called \"mean target encoding\").  With target encoding, each category is replaced with the mean target value for samples having that category.  The \"target value\" is the y-variable, or the value our model is trying to predict.  This allows us to encode an arbitrairy number of categories without increasing the dimensionality of our data!  \n",
        "\n",
        "Of course, there are drawbacks to target encoding as well.  Target encoding introduces noise into the encoding of the categorical variables (noise which comes from the noise in the target variable itself).  Also, naively applying target encoding can allow data leakage, leading to overfitting and poor predictive performance.  To fix that problem, we'll have to construct target encoders which prevent data leakage.  And even with those leak-proof target encoders, there are situations where one would be better off using one-hot or other encoding methods.  One-hot can be better in situations with few categories, or with data where there are strong interaction effects.\n",
        "\n",
        "In this post we'll evaluate different encoding schemes, build a cross-fold target encoder to mitigate the drawbacks of the naive target encoder, and determine how the performance of predictive models change based on the type of category encoding used, the number of categories in the dataset, and the presence of interaction effects. \n",
        "\n",
        "TODO: outline\n",
        "\n",
        "First let's import the packages we'll be using."
      ]
    },
    {
      "metadata": {
        "id": "fXS8EKjT2o7n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error, make_scorer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import KFold \n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "from sklearn.linear_model import BayesianRidge\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "np.random.seed(12345)\n",
        "\n",
        "import warnings\n",
        "from sklearn.exceptions import DataConversionWarning\n",
        "warnings.filterwarnings(action='ignore', category=DataConversionWarning)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8LQUOJ8D2q6f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data\n",
        "\n",
        "To evaluate the effectiveness of different encoding algorithms, we'll want to be able to generate data with different numbers of samples, features, and categories.  Let's make a function to generate categorical datasets, which allows us to set these different aspects of the data.  The categories have a direct effect on the target variable which we'll try to predict."
      ]
    },
    {
      "metadata": {
        "id": "31A0dwV-Lohj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def make_categorical_regression(n_samples=100,\n",
        "                                n_features=10,\n",
        "                                n_informative=10,\n",
        "                                n_categories=10,\n",
        "                                imbalance=0.0,\n",
        "                                noise=1.0,\n",
        "                                n_cont_features=0,\n",
        "                                cont_weight=0.1,\n",
        "                                interactions=0.0):\n",
        "    \"\"\"Generate a regression problem with categorical features.\n",
        "  \n",
        "    Parameters\n",
        "    ----------\n",
        "    n_samples : int > 0\n",
        "        Number of samples to generate\n",
        "        Default = 100\n",
        "    n_features : int > 0\n",
        "        Number of categorical features to generate\n",
        "        Default = 10\n",
        "    n_informative : int >= 0\n",
        "        Number of features which carry information about the target.\n",
        "        Default = 10\n",
        "    n_categories : int > 0\n",
        "        Number of categories per feature.  Default = 10\n",
        "    imbalance : float > 0\n",
        "        How much imbalance there is in the number of occurrences of\n",
        "        each category.  Larger values yield a higher concentration\n",
        "        of samples in only a few categories.  An imbalance of 0 \n",
        "        yields the same number of samples in each category.\n",
        "        Default = 0.0\n",
        "    noise : float > 0\n",
        "        Noise to add to target.  Default = 1.0\n",
        "    n_cont_features : int >= 0\n",
        "        Number of continuous (non-categorical) features.\n",
        "        Default = 0\n",
        "    cont_weight : float > 0\n",
        "        Weight of the continuous variables' effect.\n",
        "        Default = 0.1\n",
        "    interactions : float >= 0 and <= 1\n",
        "        Proportion of the variance due to interaction effects.\n",
        "        Note that this only adds interaction effects between the \n",
        "        categorical features, not the continuous features.\n",
        "        Default = 0.0\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    X : pandas DataFrame\n",
        "        Features.  Of shape (n_samples, n_features+n_cont_features)\n",
        "    y : pandas Series of shape (n_samples,)\n",
        "        Target variable.\n",
        "    \"\"\"\n",
        "    \n",
        "    \n",
        "    def beta_binomial(n, a, b):\n",
        "        \"\"\"Beta-binomial probability mass function.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        n : int\n",
        "            Number of trials\n",
        "        a : float > 0\n",
        "            Alpha parameter\n",
        "        b : float > 0\n",
        "            Beta parameter\n",
        "            \n",
        "        Returns\n",
        "        -------\n",
        "        ndarray of size (n,)\n",
        "            Probability mass function.\n",
        "        \"\"\"\n",
        "        from scipy.special import beta\n",
        "        from scipy.misc import comb\n",
        "        k = np.arange(n+1)\n",
        "        return comb(n, k)*beta(k+a, n-k+b)/beta(a, b)\n",
        "\n",
        "\n",
        "    # Check inputs\n",
        "    if not isinstance(n_samples, int):\n",
        "        raise TypeError('n_samples must be an int')\n",
        "    if n_samples < 1:\n",
        "        raise ValueError('n_samples must be one or greater')\n",
        "    if not isinstance(n_features, int):\n",
        "        raise TypeError('n_features must be an int')\n",
        "    if n_features < 1:\n",
        "        raise ValueError('n_features must be one or greater')\n",
        "    if not isinstance(n_informative, int):\n",
        "        raise TypeError('n_informative must be an int')\n",
        "    if n_informative < 0:\n",
        "        raise ValueError('n_informative must be non-negative')\n",
        "    if not isinstance(n_categories, int):\n",
        "        raise TypeError('n_categories must be an int')\n",
        "    if n_categories < 1:\n",
        "        raise ValueError('n_categories must be one or greater')\n",
        "    if not isinstance(imbalance, float):\n",
        "        raise TypeError('imbalance must be a float')\n",
        "    if imbalance < 0:\n",
        "        raise ValueError('imbalance must be non-negative')\n",
        "    if not isinstance(noise, float):\n",
        "        raise TypeError('noise must be a float')\n",
        "    if noise < 0:\n",
        "        raise ValueError('noise must be positive')\n",
        "    if not isinstance(n_cont_features, int):\n",
        "        raise TypeError('n_cont_features must be an int')\n",
        "    if n_cont_features < 0:\n",
        "        raise ValueError('n_cont_features must be non-negative')\n",
        "    if not isinstance(cont_weight, float):\n",
        "        raise TypeError('cont_weight must be a float')\n",
        "    if cont_weight < 0:\n",
        "        raise ValueError('cont_weight must be non-negative')\n",
        "    if not isinstance(interactions, float):\n",
        "        raise TypeError('interactions must be a float')\n",
        "    if interactions < 0:\n",
        "        raise ValueError('interactions must be non-negative')\n",
        "        \n",
        "    # Generate random categorical data (using category probabilities\n",
        "    # drawn from a beta-binomial dist w/ alpha=1, beta=imbalance+1)\n",
        "    cat_probs = beta_binomial(n_categories-1, 1.0, imbalance+1)\n",
        "    categories = np.empty((n_samples, n_features), dtype='uint64')\n",
        "    for iC in range(n_features):\n",
        "        categories[:,iC] = np.random.choice(np.arange(n_categories),\n",
        "                                            size=n_samples,\n",
        "                                            p=cat_probs)\n",
        "        \n",
        "    # Generate random values for each category\n",
        "    cat_vals = np.random.randn(n_categories, n_features)\n",
        "    \n",
        "    # Set non-informative columns' effect to 0\n",
        "    cat_vals[:,:(n_features-n_informative)] = 0\n",
        "    \n",
        "    # Compute target variable from those categories and their values\n",
        "    y = np.zeros(n_samples)\n",
        "    for iC in range(n_features):\n",
        "        y += (1.0-interactions) * cat_vals[categories[:,iC], iC]\n",
        "      \n",
        "    # Add interaction effects\n",
        "    if interactions > 0:\n",
        "        for iC1 in range(n_informative):\n",
        "            for iC2 in range(iC1+1, n_informative):\n",
        "                int_vals = np.random.randn(n_categories, #interaction\n",
        "                                           n_categories) #effects\n",
        "                y += interactions * int_vals[categories[:,iC1],\n",
        "                                             categories[:,iC2]]\n",
        "    \n",
        "    # Add noise\n",
        "    y += noise*np.random.randn(n_samples)\n",
        "    \n",
        "    # Generate dataframe from categories\n",
        "    cat_strs = [''.join([chr(ord(c)+49) for c in str(n)]) \n",
        "                for n in range(n_categories)]\n",
        "    X = pd.DataFrame()\n",
        "    for iC in range(n_features):\n",
        "        col_str = 'categorical_'+str(iC)\n",
        "        X[col_str] = [cat_strs[i] for i in categories[:,iC]]\n",
        "        \n",
        "    # Add continuous features\n",
        "    for iC in range(n_cont_features):\n",
        "        col_str = 'continuous_'+str(iC)\n",
        "        X[col_str] = cont_weight*np.random.randn(n_samples)\n",
        "        y += np.random.randn()*X[col_str]\n",
        "                    \n",
        "    # Generate series from target\n",
        "    y = pd.Series(data=y, index=X.index)\n",
        "    \n",
        "    # Return features and target\n",
        "    return X, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AwugyiOT3xgn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, we can easily generate data to test our encoders on:"
      ]
    },
    {
      "metadata": {
        "id": "RnHHmJlIw-AK",
        "colab_type": "code",
        "outputId": "7df6cd41-dca9-42eb-c13d-b0fae90bb554",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "# Generate categorical data and target\n",
        "X, y = make_categorical_regression(n_samples=2000,\n",
        "                                   n_features=10,\n",
        "                                   n_categories=100,\n",
        "                                   n_informative=1,\n",
        "                                   imbalance=2.0)\n",
        "\n",
        "# Split into test and training data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.5)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:72: DeprecationWarning: `comb` is deprecated!\n",
            "Importing `comb` from scipy.misc is deprecated in scipy 1.0.0. Use `scipy.special.comb` instead.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "XP8jiuvu4InP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The ten features in the dataset we generated are all categorical:"
      ]
    },
    {
      "metadata": {
        "id": "iBiFTq5S4Mtn",
        "colab_type": "code",
        "outputId": "a4b8a696-ba83-45f3-ec35-27b48543852b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        }
      },
      "cell_type": "code",
      "source": [
        "X_train.sample(10)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>categorical_0</th>\n",
              "      <th>categorical_1</th>\n",
              "      <th>categorical_2</th>\n",
              "      <th>categorical_3</th>\n",
              "      <th>categorical_4</th>\n",
              "      <th>categorical_5</th>\n",
              "      <th>categorical_6</th>\n",
              "      <th>categorical_7</th>\n",
              "      <th>categorical_8</th>\n",
              "      <th>categorical_9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>792</th>\n",
              "      <td>cf</td>\n",
              "      <td>c</td>\n",
              "      <td>d</td>\n",
              "      <td>a</td>\n",
              "      <td>ed</td>\n",
              "      <td>ca</td>\n",
              "      <td>dj</td>\n",
              "      <td>g</td>\n",
              "      <td>b</td>\n",
              "      <td>bj</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>276</th>\n",
              "      <td>di</td>\n",
              "      <td>b</td>\n",
              "      <td>bd</td>\n",
              "      <td>fg</td>\n",
              "      <td>d</td>\n",
              "      <td>j</td>\n",
              "      <td>e</td>\n",
              "      <td>a</td>\n",
              "      <td>hc</td>\n",
              "      <td>h</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1016</th>\n",
              "      <td>ei</td>\n",
              "      <td>di</td>\n",
              "      <td>cj</td>\n",
              "      <td>he</td>\n",
              "      <td>hb</td>\n",
              "      <td>gh</td>\n",
              "      <td>b</td>\n",
              "      <td>bh</td>\n",
              "      <td>df</td>\n",
              "      <td>c</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1372</th>\n",
              "      <td>ca</td>\n",
              "      <td>c</td>\n",
              "      <td>be</td>\n",
              "      <td>ce</td>\n",
              "      <td>cg</td>\n",
              "      <td>bf</td>\n",
              "      <td>de</td>\n",
              "      <td>fe</td>\n",
              "      <td>ba</td>\n",
              "      <td>fd</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1860</th>\n",
              "      <td>db</td>\n",
              "      <td>dh</td>\n",
              "      <td>ba</td>\n",
              "      <td>bh</td>\n",
              "      <td>di</td>\n",
              "      <td>bh</td>\n",
              "      <td>db</td>\n",
              "      <td>bi</td>\n",
              "      <td>gf</td>\n",
              "      <td>bi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1431</th>\n",
              "      <td>h</td>\n",
              "      <td>ce</td>\n",
              "      <td>ea</td>\n",
              "      <td>i</td>\n",
              "      <td>eb</td>\n",
              "      <td>g</td>\n",
              "      <td>da</td>\n",
              "      <td>da</td>\n",
              "      <td>fc</td>\n",
              "      <td>e</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>328</th>\n",
              "      <td>j</td>\n",
              "      <td>db</td>\n",
              "      <td>df</td>\n",
              "      <td>fa</td>\n",
              "      <td>fe</td>\n",
              "      <td>g</td>\n",
              "      <td>c</td>\n",
              "      <td>h</td>\n",
              "      <td>da</td>\n",
              "      <td>bg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1708</th>\n",
              "      <td>cd</td>\n",
              "      <td>ci</td>\n",
              "      <td>f</td>\n",
              "      <td>be</td>\n",
              "      <td>e</td>\n",
              "      <td>fb</td>\n",
              "      <td>dc</td>\n",
              "      <td>bi</td>\n",
              "      <td>ec</td>\n",
              "      <td>da</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1567</th>\n",
              "      <td>ei</td>\n",
              "      <td>cj</td>\n",
              "      <td>ch</td>\n",
              "      <td>bc</td>\n",
              "      <td>bb</td>\n",
              "      <td>f</td>\n",
              "      <td>ch</td>\n",
              "      <td>bi</td>\n",
              "      <td>c</td>\n",
              "      <td>he</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1027</th>\n",
              "      <td>bi</td>\n",
              "      <td>bh</td>\n",
              "      <td>bf</td>\n",
              "      <td>ba</td>\n",
              "      <td>dc</td>\n",
              "      <td>da</td>\n",
              "      <td>g</td>\n",
              "      <td>cc</td>\n",
              "      <td>bi</td>\n",
              "      <td>ee</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     categorical_0 categorical_1 categorical_2 categorical_3 categorical_4  \\\n",
              "792             cf             c             d             a            ed   \n",
              "276             di             b            bd            fg             d   \n",
              "1016            ei            di            cj            he            hb   \n",
              "1372            ca             c            be            ce            cg   \n",
              "1860            db            dh            ba            bh            di   \n",
              "1431             h            ce            ea             i            eb   \n",
              "328              j            db            df            fa            fe   \n",
              "1708            cd            ci             f            be             e   \n",
              "1567            ei            cj            ch            bc            bb   \n",
              "1027            bi            bh            bf            ba            dc   \n",
              "\n",
              "     categorical_5 categorical_6 categorical_7 categorical_8 categorical_9  \n",
              "792             ca            dj             g             b            bj  \n",
              "276              j             e             a            hc             h  \n",
              "1016            gh             b            bh            df             c  \n",
              "1372            bf            de            fe            ba            fd  \n",
              "1860            bh            db            bi            gf            bi  \n",
              "1431             g            da            da            fc             e  \n",
              "328              g             c             h            da            bg  \n",
              "1708            fb            dc            bi            ec            da  \n",
              "1567             f            ch            bi             c            he  \n",
              "1027            da             g            cc            bi            ee  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "295Wd_B1zT9a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Using the pandas package, these are stored as the \"object\" datatype:"
      ]
    },
    {
      "metadata": {
        "id": "yxaiV1qjnpJb",
        "colab_type": "code",
        "outputId": "ed3c1cc6-99f3-489b-a915-b28585129fa7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "cell_type": "code",
      "source": [
        "X_train.info()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 1000 entries, 523 to 583\n",
            "Data columns (total 10 columns):\n",
            "categorical_0    1000 non-null object\n",
            "categorical_1    1000 non-null object\n",
            "categorical_2    1000 non-null object\n",
            "categorical_3    1000 non-null object\n",
            "categorical_4    1000 non-null object\n",
            "categorical_5    1000 non-null object\n",
            "categorical_6    1000 non-null object\n",
            "categorical_7    1000 non-null object\n",
            "categorical_8    1000 non-null object\n",
            "categorical_9    1000 non-null object\n",
            "dtypes: object(10)\n",
            "memory usage: 85.9+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ckap_xbm-ZFo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "While all the features are categorical, the target variable is continuous:"
      ]
    },
    {
      "metadata": {
        "id": "y_82h44JyBW7",
        "colab_type": "code",
        "outputId": "08a87fb9-1cc6-4c41-f4e1-e8dad0e7d0a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "cell_type": "code",
      "source": [
        "y_train.hist(bins=20)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f7abccf8e80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeQAAAFKCAYAAADMuCxnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAwgyAAAGECQAQAwgCADAGDA/wPbMhbj+IXs0AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "sH3ADlvjAbo6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now the question is: which encoding scheme best allows us to glean the most information from the categorical features, leading to the best predictions of the target variable?"
      ]
    },
    {
      "metadata": {
        "id": "7gsbU-a4ArVl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Baseline\n",
        "\n",
        "For comparison, how well would we do if we just predicted the mean target value for all samples?  We'll use the mean absolute error (MAE) as our performance metric."
      ]
    },
    {
      "metadata": {
        "id": "UTQOgHegAqhh",
        "colab_type": "code",
        "outputId": "901b4494-5a3c-482e-9e4e-5834628245c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "mean_absolute_error(y_train, \n",
        "                    np.full(y_train.shape[0], y_train.mean()))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.139564825988808"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "tPKR7mZICTrT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "So our predictive models should definitely be shooting for a mean absolute error of less than that!  But, we added random noise with a standard deviation of 1, so even if our model is *perfect*, the best MAE we can expect is:"
      ]
    },
    {
      "metadata": {
        "id": "L2dIB7n4tLNK",
        "colab_type": "code",
        "outputId": "476d97bb-731d-43ac-f995-776b1922e03d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "mean_absolute_error(np.random.randn(10000), \n",
        "                    np.zeros(10000))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7995403442995148"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "F3spAaSlLozj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Label Encoding\n",
        "\n",
        "TODO: prediction w/ simple label encoding, which is just replacing each unique category w/ a unique integer\n",
        "\n",
        "TODO: note that we could also use Scikit-learn's LabelEncoder"
      ]
    },
    {
      "metadata": {
        "id": "pFPw2LQZggHr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class LabelEncoder(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Label encoder.\n",
        "    \n",
        "    Replaces categorical column(s) with integer labels for each unique\n",
        "    category in original column.\n",
        "\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, cols=None):\n",
        "        \"\"\"Label encoder.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        cols : list of str\n",
        "            Columns to label encode.  Default is to label encode \n",
        "            all categorical columns in the DataFrame.\n",
        "        \"\"\"\n",
        "        if isinstance(cols, str):\n",
        "            self.cols = [cols]\n",
        "        else:\n",
        "            self.cols = cols\n",
        "        \n",
        "        \n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Fit label encoder to X and y\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        X : pandas DataFrame, shape [n_samples, n_columns]\n",
        "            DataFrame containing columns to label encode\n",
        "        y : pandas Series, shape = [n_samples]\n",
        "            Target values.\n",
        "            \n",
        "        Returns\n",
        "        -------\n",
        "        self : encoder\n",
        "            Returns self.\n",
        "        \"\"\"\n",
        "        \n",
        "        # Encode all categorical cols by default\n",
        "        if self.cols is None:\n",
        "            self.cols = [c for c in X if str(X[c].dtype)=='object']\n",
        "\n",
        "        # Check columns are in X\n",
        "        for col in self.cols:\n",
        "            if col not in X:\n",
        "                raise ValueError('Column \\''+col+'\\' not in X')\n",
        "\n",
        "        # Create the map from objects to integers for each column\n",
        "        self.maps = dict() #dict to store map for each column\n",
        "        for col in self.cols:\n",
        "            self.maps[col] = dict(zip(\n",
        "                X[col].values, \n",
        "                X[col].astype('category').cat.codes.values\n",
        "            ))\n",
        "                        \n",
        "        # Return fit object\n",
        "        return self\n",
        "\n",
        "        \n",
        "    def transform(self, X, y=None):\n",
        "        \"\"\"Perform the label encoding transformation.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        X : pandas DataFrame, shape [n_samples, n_columns]\n",
        "            DataFrame containing columns to label encode\n",
        "            \n",
        "        Returns\n",
        "        -------\n",
        "        pandas DataFrame\n",
        "            Input DataFrame with transformed columns\n",
        "        \"\"\"\n",
        "        Xo = X.copy()\n",
        "        for col, tmap in self.maps.items():\n",
        "          \n",
        "            # Map the column\n",
        "            Xo[col] = Xo[col].map(tmap)\n",
        "            \n",
        "            # Convert to appropriate datatype\n",
        "            max_val = max(tmap.values())\n",
        "            if Xo[col].isnull().any(): #nulls, so have to use float!\n",
        "                if max_val < 8388608:\n",
        "                    dtype = 'float32'\n",
        "                else:\n",
        "                    dtype = 'float64'\n",
        "            else:\n",
        "                if max_val < 256:\n",
        "                    dtype = 'uint8'\n",
        "                elif max_val < 65536:\n",
        "                    dtype = 'uint16'\n",
        "                elif max_val < 4294967296:\n",
        "                    dtype = 'uint32'\n",
        "                else:\n",
        "                    dtype = 'uint64'\n",
        "            Xo[col] = Xo[col].astype(dtype)\n",
        "            \n",
        "        # Return encoded dataframe\n",
        "        return Xo\n",
        "            \n",
        "            \n",
        "    def fit_transform(self, X, y=None):\n",
        "        \"\"\"Fit and transform the data via label encoding.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        X : pandas DataFrame, shape [n_samples, n_columns]\n",
        "            DataFrame containing columns to label encode\n",
        "        y : pandas Series, shape = [n_samples]\n",
        "            Target values\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        pandas DataFrame\n",
        "            Input DataFrame with transformed columns\n",
        "        \"\"\"\n",
        "        return self.fit(X, y).transform(X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nDhwfsV5DDpj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we can convert the categories to integers:"
      ]
    },
    {
      "metadata": {
        "id": "Y2YJidsuC0if",
        "colab_type": "code",
        "outputId": "5a7dd427-e38b-42ed-86bd-30ecb2b72702",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        }
      },
      "cell_type": "code",
      "source": [
        "# Label encode the categorical data\n",
        "le = LabelEncoder()\n",
        "X_label_encoded = le.fit_transform(X_train, y_train)\n",
        "X_label_encoded.sample(10)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>categorical_0</th>\n",
              "      <th>categorical_1</th>\n",
              "      <th>categorical_2</th>\n",
              "      <th>categorical_3</th>\n",
              "      <th>categorical_4</th>\n",
              "      <th>categorical_5</th>\n",
              "      <th>categorical_6</th>\n",
              "      <th>categorical_7</th>\n",
              "      <th>categorical_8</th>\n",
              "      <th>categorical_9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>884</th>\n",
              "      <td>13</td>\n",
              "      <td>40</td>\n",
              "      <td>83</td>\n",
              "      <td>5</td>\n",
              "      <td>82</td>\n",
              "      <td>32</td>\n",
              "      <td>44</td>\n",
              "      <td>6</td>\n",
              "      <td>23</td>\n",
              "      <td>55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1098</th>\n",
              "      <td>15</td>\n",
              "      <td>66</td>\n",
              "      <td>34</td>\n",
              "      <td>70</td>\n",
              "      <td>56</td>\n",
              "      <td>36</td>\n",
              "      <td>69</td>\n",
              "      <td>3</td>\n",
              "      <td>86</td>\n",
              "      <td>48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>853</th>\n",
              "      <td>56</td>\n",
              "      <td>6</td>\n",
              "      <td>12</td>\n",
              "      <td>30</td>\n",
              "      <td>27</td>\n",
              "      <td>25</td>\n",
              "      <td>56</td>\n",
              "      <td>10</td>\n",
              "      <td>54</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1667</th>\n",
              "      <td>17</td>\n",
              "      <td>6</td>\n",
              "      <td>77</td>\n",
              "      <td>8</td>\n",
              "      <td>22</td>\n",
              "      <td>65</td>\n",
              "      <td>22</td>\n",
              "      <td>33</td>\n",
              "      <td>5</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1136</th>\n",
              "      <td>56</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>34</td>\n",
              "      <td>0</td>\n",
              "      <td>31</td>\n",
              "      <td>48</td>\n",
              "      <td>80</td>\n",
              "      <td>50</td>\n",
              "      <td>56</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>362</th>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>47</td>\n",
              "      <td>50</td>\n",
              "      <td>42</td>\n",
              "      <td>7</td>\n",
              "      <td>9</td>\n",
              "      <td>3</td>\n",
              "      <td>81</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1977</th>\n",
              "      <td>21</td>\n",
              "      <td>19</td>\n",
              "      <td>22</td>\n",
              "      <td>56</td>\n",
              "      <td>56</td>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>43</td>\n",
              "      <td>13</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1784</th>\n",
              "      <td>35</td>\n",
              "      <td>66</td>\n",
              "      <td>37</td>\n",
              "      <td>66</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>76</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127</th>\n",
              "      <td>82</td>\n",
              "      <td>42</td>\n",
              "      <td>11</td>\n",
              "      <td>63</td>\n",
              "      <td>12</td>\n",
              "      <td>39</td>\n",
              "      <td>58</td>\n",
              "      <td>76</td>\n",
              "      <td>59</td>\n",
              "      <td>67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1489</th>\n",
              "      <td>2</td>\n",
              "      <td>66</td>\n",
              "      <td>35</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>45</td>\n",
              "      <td>24</td>\n",
              "      <td>23</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      categorical_0  categorical_1  categorical_2  categorical_3  \\\n",
              "884              13             40             83              5   \n",
              "1098             15             66             34             70   \n",
              "853              56              6             12             30   \n",
              "1667             17              6             77              8   \n",
              "1136             56              5              8             34   \n",
              "362               0             15             47             50   \n",
              "1977             21             19             22             56   \n",
              "1784             35             66             37             66   \n",
              "127              82             42             11             63   \n",
              "1489              2             66             35              1   \n",
              "\n",
              "      categorical_4  categorical_5  categorical_6  categorical_7  \\\n",
              "884              82             32             44              6   \n",
              "1098             56             36             69              3   \n",
              "853              27             25             56             10   \n",
              "1667             22             65             22             33   \n",
              "1136              0             31             48             80   \n",
              "362              42              7              9              3   \n",
              "1977             56             13              1             43   \n",
              "1784              4              1             76              8   \n",
              "127              12             39             58             76   \n",
              "1489              4              1             45             24   \n",
              "\n",
              "      categorical_8  categorical_9  \n",
              "884              23             55  \n",
              "1098             86             48  \n",
              "853              54              6  \n",
              "1667              5             22  \n",
              "1136             50             56  \n",
              "362              81             34  \n",
              "1977             13             34  \n",
              "1784              1             10  \n",
              "127              59             67  \n",
              "1489             23             24  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "67LwHzQAW1Rl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "But again, these integers aren't related to the categories in any meaningful way - aside from the fact that each unique integer corresponds to a unique category.\n",
        "\n",
        "TODO: then we can make a model to predict"
      ]
    },
    {
      "metadata": {
        "id": "u4Nibf2SW061",
        "colab_type": "code",
        "outputId": "c283fb55-defa-4b1e-ea57-d782eed685a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Regression model\n",
        "model_le = Pipeline([\n",
        "    ('label-encoder', LabelEncoder()),\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('regressor', BayesianRidge())\n",
        "])\n",
        "\n",
        "# Cross-validated MAE\n",
        "mae_scorer = make_scorer(mean_absolute_error)\n",
        "scores = cross_val_score(model_le, X_train, y_train, \n",
        "                         cv=3, scoring=mae_scorer)\n",
        "print('Cross-validated MAE:', scores.mean(), '+/-', scores.std())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cross-validated MAE: 1.13197409773137 +/- 0.021768461339780148\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0FU8xUdnbdX3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "TODO: That's not much better than just predicting the mean!\n",
        "\n",
        "However, the error is no worse on the test data than the cross-validated error on the training data.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "m9Depw3LbhbG",
        "colab_type": "code",
        "outputId": "2b396337-4602-45b5-bbef-fe6759412f83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# MAE on test data\n",
        "model_le.fit(X_train, y_train)\n",
        "y_pred = model_le.predict(X_test)\n",
        "test_mae = mean_absolute_error(y_test, y_pred)\n",
        "print('Validation MAE:', test_mae)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation MAE: 1.175866100844089\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "g__EEWRPLuEe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## One-hot Encoding\n",
        "\n",
        "TODO: one-hot encoding, sometimes aka \"Dummy encoding\"\n",
        "\n",
        "(note that we could also use Scikit-learn's OneHotEncoder)"
      ]
    },
    {
      "metadata": {
        "id": "xu7mOQTedGsp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class OneHotEncoder(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"One-hot encoder.\n",
        "    \n",
        "    Replaces categorical column(s) with binary columns for each unique\n",
        "    value in original column.\n",
        "\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, cols=None, reduce_df=False):\n",
        "        \"\"\"One-hot encoder.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        cols : list of str\n",
        "            Columns to one-hot encode.  Default is to one-hot encode \n",
        "            all categorical columns in the DataFrame.\n",
        "        reduce_df : bool\n",
        "            Whether to add N-1 one-hot columns for a column with N \n",
        "            categories. E.g. for a column with categories A, B, and C:\n",
        "            When reduce_df is True, A=[1, 0], B=[0, 1], and C=[0, 0]\n",
        "            When reduce_df is False, A=[1, 0, 0], B=[0, 1, 0], and \n",
        "            C=[0, 0, 1]\n",
        "            Default = False\n",
        "        \"\"\"\n",
        "        if isinstance(cols, str):\n",
        "            self.cols = [cols]\n",
        "        else:\n",
        "            self.cols = cols\n",
        "        self.reduce_df = reduce_df\n",
        "        \n",
        "        \n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Fit one-hot encoder to X and y\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        X : pandas DataFrame, shape [n_samples, n_columns]\n",
        "            DataFrame containing columns to encode\n",
        "        y : pandas Series, shape = [n_samples]\n",
        "            Target values.\n",
        "            \n",
        "        Returns\n",
        "        -------\n",
        "        self : encoder\n",
        "            Returns self.\n",
        "        \"\"\"\n",
        "        \n",
        "        # Encode all categorical cols by default\n",
        "        if self.cols is None:\n",
        "            self.cols = [c for c in X if str(X[c].dtype)=='object']\n",
        "\n",
        "        # Check columns are in X\n",
        "        for col in self.cols:\n",
        "            if col not in X:\n",
        "                raise ValueError('Column \\''+col+'\\' not in X')\n",
        "\n",
        "        # Store each unique value\n",
        "        self.maps = dict() #dict to store map for each column\n",
        "        for col in self.cols:\n",
        "            self.maps[col] = []\n",
        "            uniques = X[col].unique()\n",
        "            for unique in uniques:\n",
        "                self.maps[col].append(unique)\n",
        "            if self.reduce_df:\n",
        "                del self.maps[col][-1]\n",
        "        \n",
        "        # Return fit object\n",
        "        return self\n",
        "\n",
        "        \n",
        "    def transform(self, X, y=None):\n",
        "        \"\"\"Perform the one-hot encoding transformation.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        X : pandas DataFrame, shape [n_samples, n_columns]\n",
        "            DataFrame containing columns to one-hot encode\n",
        "            \n",
        "        Returns\n",
        "        -------\n",
        "        pandas DataFrame\n",
        "            Input DataFrame with transformed columns\n",
        "        \"\"\"\n",
        "        Xo = X.copy()\n",
        "        for col, vals in self.maps.items():\n",
        "            for val in vals:\n",
        "                new_col = col+'_'+str(val)\n",
        "                Xo[new_col] = (Xo[col]==val).astype('uint8')\n",
        "            del Xo[col]\n",
        "        return Xo\n",
        "            \n",
        "            \n",
        "    def fit_transform(self, X, y=None):\n",
        "        \"\"\"Fit and transform the data via one-hot encoding.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        X : pandas DataFrame, shape [n_samples, n_columns]\n",
        "            DataFrame containing columns to one-hot encode\n",
        "        y : pandas Series, shape = [n_samples]\n",
        "            Target values\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        pandas DataFrame\n",
        "            Input DataFrame with transformed columns\n",
        "        \"\"\"\n",
        "        return self.fit(X, y).transform(X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AosYxSrbNBOs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, instead of replacing categories with integer labels, we've create a new column for each category in each original column.  The value in a given column is 1 when the original category matches, otherwise the value is 0."
      ]
    },
    {
      "metadata": {
        "id": "Xg9TlgHDNRC-",
        "colab_type": "code",
        "outputId": "4701c95e-f101-445b-8b7e-3a01de3618be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "cell_type": "code",
      "source": [
        "# One-hot-encode the categorical data\n",
        "ohe = OneHotEncoder()\n",
        "X_one_hot = ohe.fit_transform(X_train, y_train)\n",
        "X_one_hot.sample(10)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>categorical_0_ec</th>\n",
              "      <th>categorical_0_ba</th>\n",
              "      <th>categorical_0_bg</th>\n",
              "      <th>categorical_0_b</th>\n",
              "      <th>categorical_0_h</th>\n",
              "      <th>categorical_0_j</th>\n",
              "      <th>categorical_0_ge</th>\n",
              "      <th>categorical_0_cg</th>\n",
              "      <th>categorical_0_fh</th>\n",
              "      <th>categorical_0_dc</th>\n",
              "      <th>...</th>\n",
              "      <th>categorical_9_ga</th>\n",
              "      <th>categorical_9_eb</th>\n",
              "      <th>categorical_9_gg</th>\n",
              "      <th>categorical_9_hj</th>\n",
              "      <th>categorical_9_gi</th>\n",
              "      <th>categorical_9_he</th>\n",
              "      <th>categorical_9_ff</th>\n",
              "      <th>categorical_9_hb</th>\n",
              "      <th>categorical_9_gd</th>\n",
              "      <th>categorical_9_fi</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>294</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>900</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>443</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1268</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>280</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>382</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>624</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>430</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>836</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows  853 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      categorical_0_ec  categorical_0_ba  categorical_0_bg  categorical_0_b  \\\n",
              "294                  0                 0                 0                0   \n",
              "900                  0                 0                 0                0   \n",
              "39                   0                 0                 0                0   \n",
              "443                  0                 0                 0                0   \n",
              "1268                 0                 0                 0                0   \n",
              "280                  0                 0                 0                0   \n",
              "382                  0                 0                 0                0   \n",
              "624                  0                 0                 0                0   \n",
              "430                  0                 0                 0                0   \n",
              "836                  0                 0                 0                0   \n",
              "\n",
              "      categorical_0_h  categorical_0_j  categorical_0_ge  categorical_0_cg  \\\n",
              "294                 0                0                 0                 0   \n",
              "900                 1                0                 0                 0   \n",
              "39                  0                0                 0                 0   \n",
              "443                 0                0                 0                 0   \n",
              "1268                0                0                 0                 0   \n",
              "280                 0                0                 0                 0   \n",
              "382                 0                0                 0                 0   \n",
              "624                 0                0                 0                 0   \n",
              "430                 0                0                 0                 0   \n",
              "836                 0                0                 0                 0   \n",
              "\n",
              "      categorical_0_fh  categorical_0_dc        ...         categorical_9_ga  \\\n",
              "294                  0                 0        ...                        0   \n",
              "900                  0                 0        ...                        0   \n",
              "39                   0                 0        ...                        0   \n",
              "443                  0                 0        ...                        0   \n",
              "1268                 0                 0        ...                        0   \n",
              "280                  0                 0        ...                        0   \n",
              "382                  0                 0        ...                        0   \n",
              "624                  0                 0        ...                        0   \n",
              "430                  0                 0        ...                        0   \n",
              "836                  0                 0        ...                        0   \n",
              "\n",
              "      categorical_9_eb  categorical_9_gg  categorical_9_hj  categorical_9_gi  \\\n",
              "294                  0                 0                 0                 0   \n",
              "900                  0                 0                 0                 0   \n",
              "39                   0                 0                 0                 0   \n",
              "443                  0                 0                 0                 0   \n",
              "1268                 0                 0                 0                 0   \n",
              "280                  0                 0                 0                 0   \n",
              "382                  0                 0                 0                 0   \n",
              "624                  0                 0                 0                 0   \n",
              "430                  0                 0                 0                 0   \n",
              "836                  0                 0                 0                 0   \n",
              "\n",
              "      categorical_9_he  categorical_9_ff  categorical_9_hb  categorical_9_gd  \\\n",
              "294                  0                 0                 0                 0   \n",
              "900                  0                 0                 0                 0   \n",
              "39                   0                 0                 0                 0   \n",
              "443                  0                 0                 0                 0   \n",
              "1268                 0                 0                 0                 0   \n",
              "280                  0                 0                 0                 0   \n",
              "382                  0                 0                 0                 0   \n",
              "624                  0                 0                 0                 0   \n",
              "430                  0                 0                 0                 0   \n",
              "836                  0                 0                 0                 0   \n",
              "\n",
              "      categorical_9_fi  \n",
              "294                  0  \n",
              "900                  0  \n",
              "39                   0  \n",
              "443                  0  \n",
              "1268                 0  \n",
              "280                  0  \n",
              "382                  0  \n",
              "624                  0  \n",
              "430                  0  \n",
              "836                  0  \n",
              "\n",
              "[10 rows x 853 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "8HD4uwYjNbas",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can fit the same model with the one-hot encoded data as we fit to the label-encoded data."
      ]
    },
    {
      "metadata": {
        "id": "-Bxu2FRyfqKb",
        "colab_type": "code",
        "outputId": "016fb2e2-153c-4f03-df20-5f8d022dc43f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Regression model\n",
        "model_oh = Pipeline([\n",
        "    ('encoder', OneHotEncoder()),\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('regressor', BayesianRidge())\n",
        "])\n",
        "\n",
        "# Cross-validated MAE\n",
        "scores = cross_val_score(model_oh, X_train, y_train, \n",
        "                         cv=3, scoring=mae_scorer)\n",
        "print('Cross-validated MAE:', scores.mean(), '+/-', scores.std())"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cross-validated MAE: 1.0386222429286203 +/- 0.028114652268138147\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Hu8cQWbsOHHI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "TODO: better than just guessing the mean, but not by a whole lot!"
      ]
    },
    {
      "metadata": {
        "id": "T5wvQe0da6_S",
        "colab_type": "code",
        "outputId": "eda3bf51-5e68-4b5b-8d4e-34929cdc9414",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# MAE on test data\n",
        "model_oh.fit(X_train, y_train)\n",
        "y_pred = model_oh.predict(X_test)\n",
        "test_mae = mean_absolute_error(y_test, y_pred)\n",
        "print('Validation MAE:', test_mae)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation MAE: 1.028751865940599\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pJR4JtnALzk7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Target Encoding\n",
        "\n",
        "TODO: target encoding replaces categorical vals w/ the mean of the target for that category; explain w/ diagrams"
      ]
    },
    {
      "metadata": {
        "id": "eLTfbgi02ram",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class TargetEncoder(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Target encoder.\n",
        "    \n",
        "    Replaces categorical column(s) with the mean target value for\n",
        "    each category.\n",
        "\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, cols=None):\n",
        "        \"\"\"Target encoder\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        cols : list of str\n",
        "            Columns to target encode.  Default is to target encode all \n",
        "            categorical columns in the DataFrame.\n",
        "        \"\"\"\n",
        "        if isinstance(cols, str):\n",
        "            self.cols = [cols]\n",
        "        else:\n",
        "            self.cols = cols\n",
        "        \n",
        "        \n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Fit target encoder to X and y\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        X : pandas DataFrame, shape [n_samples, n_columns]\n",
        "            DataFrame containing columns to encode\n",
        "        y : pandas Series, shape = [n_samples]\n",
        "            Target values.\n",
        "            \n",
        "        Returns\n",
        "        -------\n",
        "        self : encoder\n",
        "            Returns self.\n",
        "        \"\"\"\n",
        "        \n",
        "        # Encode all categorical cols by default\n",
        "        if self.cols is None:\n",
        "            self.cols = [col for col in X if str(X[col].dtype)=='object']\n",
        "\n",
        "        # Check columns are in X\n",
        "        for col in self.cols:\n",
        "            if col not in X:\n",
        "                raise ValueError('Column \\''+col+'\\' not in X')\n",
        "\n",
        "        # Encode each element of each column\n",
        "        self.maps = dict() #dict to store map for each column\n",
        "        for col in self.cols:\n",
        "            tmap = dict()\n",
        "            uniques = X[col].unique()\n",
        "            for unique in uniques:\n",
        "                tmap[unique] = y[X[col]==unique].mean()\n",
        "            self.maps[col] = tmap\n",
        "            \n",
        "        return self\n",
        "\n",
        "        \n",
        "    def transform(self, X, y=None):\n",
        "        \"\"\"Perform the target encoding transformation.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        X : pandas DataFrame, shape [n_samples, n_columns]\n",
        "            DataFrame containing columns to encode\n",
        "            \n",
        "        Returns\n",
        "        -------\n",
        "        pandas DataFrame\n",
        "            Input DataFrame with transformed columns\n",
        "        \"\"\"\n",
        "        Xo = X.copy()\n",
        "        for col, tmap in self.maps.items():\n",
        "            vals = np.full(X.shape[0], np.nan)\n",
        "            for val, mean_target in tmap.items():\n",
        "                vals[X[col]==val] = mean_target\n",
        "            Xo[col] = vals\n",
        "        return Xo\n",
        "            \n",
        "            \n",
        "    def fit_transform(self, X, y=None):\n",
        "        \"\"\"Fit and transform the data via target encoding.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        X : pandas DataFrame, shape [n_samples, n_columns]\n",
        "            DataFrame containing columns to encode\n",
        "        y : pandas Series, shape = [n_samples]\n",
        "            Target values (required!).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        pandas DataFrame\n",
        "            Input DataFrame with transformed columns\n",
        "        \"\"\"\n",
        "        return self.fit(X, y).transform(X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_S8jg9gZVvrV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, instead of creating a bazillion columns (like with one-hot encoding), we can simply replace each category with the mean target value for that category.  This allows us to represent the categorical information in the same dimensionality, while retaining some information about the categories.  By target-encoding the features matrix, we get a matrix of the same size, but filled with continuous values instead of categories:"
      ]
    },
    {
      "metadata": {
        "id": "G3Q_B2N5V2ZN",
        "colab_type": "code",
        "outputId": "9362b038-9bc3-4e2e-81d8-0a6a14dcfd97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        }
      },
      "cell_type": "code",
      "source": [
        "# Target encode the categorical data\n",
        "te = TargetEncoder()\n",
        "X_target_encoded = te.fit_transform(X_train, y_train)\n",
        "X_target_encoded.sample(10)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>categorical_0</th>\n",
              "      <th>categorical_1</th>\n",
              "      <th>categorical_2</th>\n",
              "      <th>categorical_3</th>\n",
              "      <th>categorical_4</th>\n",
              "      <th>categorical_5</th>\n",
              "      <th>categorical_6</th>\n",
              "      <th>categorical_7</th>\n",
              "      <th>categorical_8</th>\n",
              "      <th>categorical_9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>711</th>\n",
              "      <td>-0.030636</td>\n",
              "      <td>0.192812</td>\n",
              "      <td>0.269273</td>\n",
              "      <td>0.131628</td>\n",
              "      <td>0.319769</td>\n",
              "      <td>0.190861</td>\n",
              "      <td>0.142159</td>\n",
              "      <td>0.393587</td>\n",
              "      <td>-0.766454</td>\n",
              "      <td>-0.312227</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>475</th>\n",
              "      <td>0.365423</td>\n",
              "      <td>-0.605778</td>\n",
              "      <td>-0.258930</td>\n",
              "      <td>0.038321</td>\n",
              "      <td>-0.283131</td>\n",
              "      <td>0.046135</td>\n",
              "      <td>0.316052</td>\n",
              "      <td>-0.120822</td>\n",
              "      <td>-0.425927</td>\n",
              "      <td>-0.163454</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>273</th>\n",
              "      <td>0.110462</td>\n",
              "      <td>1.093313</td>\n",
              "      <td>0.309760</td>\n",
              "      <td>0.474308</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.003120</td>\n",
              "      <td>1.558923</td>\n",
              "      <td>0.244971</td>\n",
              "      <td>-0.387846</td>\n",
              "      <td>-0.327537</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>-0.848020</td>\n",
              "      <td>0.300673</td>\n",
              "      <td>0.125095</td>\n",
              "      <td>-0.650361</td>\n",
              "      <td>-0.252932</td>\n",
              "      <td>0.293856</td>\n",
              "      <td>-0.197504</td>\n",
              "      <td>0.050085</td>\n",
              "      <td>-0.587633</td>\n",
              "      <td>-0.413439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>275</th>\n",
              "      <td>0.126068</td>\n",
              "      <td>0.180776</td>\n",
              "      <td>-0.143977</td>\n",
              "      <td>-0.131238</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>-0.760367</td>\n",
              "      <td>0.326620</td>\n",
              "      <td>-0.037488</td>\n",
              "      <td>-0.121713</td>\n",
              "      <td>-0.244310</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>336</th>\n",
              "      <td>0.543412</td>\n",
              "      <td>-0.045947</td>\n",
              "      <td>0.180144</td>\n",
              "      <td>0.279675</td>\n",
              "      <td>-0.532591</td>\n",
              "      <td>0.338287</td>\n",
              "      <td>0.071977</td>\n",
              "      <td>0.113531</td>\n",
              "      <td>0.527567</td>\n",
              "      <td>1.290724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1462</th>\n",
              "      <td>-0.065061</td>\n",
              "      <td>-0.605778</td>\n",
              "      <td>0.172445</td>\n",
              "      <td>-0.268622</td>\n",
              "      <td>-0.283131</td>\n",
              "      <td>-0.270112</td>\n",
              "      <td>-0.197504</td>\n",
              "      <td>0.068821</td>\n",
              "      <td>0.371461</td>\n",
              "      <td>0.966579</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1436</th>\n",
              "      <td>0.186988</td>\n",
              "      <td>0.564015</td>\n",
              "      <td>0.135396</td>\n",
              "      <td>0.474308</td>\n",
              "      <td>0.338006</td>\n",
              "      <td>0.479294</td>\n",
              "      <td>0.063384</td>\n",
              "      <td>0.342170</td>\n",
              "      <td>-0.054090</td>\n",
              "      <td>-0.163454</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1611</th>\n",
              "      <td>0.350326</td>\n",
              "      <td>-0.188197</td>\n",
              "      <td>-0.537682</td>\n",
              "      <td>-0.391143</td>\n",
              "      <td>0.212399</td>\n",
              "      <td>-1.811086</td>\n",
              "      <td>0.204642</td>\n",
              "      <td>-0.622682</td>\n",
              "      <td>-0.425927</td>\n",
              "      <td>-0.489371</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166</th>\n",
              "      <td>0.480637</td>\n",
              "      <td>0.975462</td>\n",
              "      <td>0.135396</td>\n",
              "      <td>-0.131238</td>\n",
              "      <td>0.323870</td>\n",
              "      <td>0.279502</td>\n",
              "      <td>-0.533179</td>\n",
              "      <td>0.050085</td>\n",
              "      <td>0.016449</td>\n",
              "      <td>1.410449</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      categorical_0  categorical_1  categorical_2  categorical_3  \\\n",
              "711       -0.030636       0.192812       0.269273       0.131628   \n",
              "475        0.365423      -0.605778      -0.258930       0.038321   \n",
              "273        0.110462       1.093313       0.309760       0.474308   \n",
              "9         -0.848020       0.300673       0.125095      -0.650361   \n",
              "275        0.126068       0.180776      -0.143977      -0.131238   \n",
              "336        0.543412      -0.045947       0.180144       0.279675   \n",
              "1462      -0.065061      -0.605778       0.172445      -0.268622   \n",
              "1436       0.186988       0.564015       0.135396       0.474308   \n",
              "1611       0.350326      -0.188197      -0.537682      -0.391143   \n",
              "166        0.480637       0.975462       0.135396      -0.131238   \n",
              "\n",
              "      categorical_4  categorical_5  categorical_6  categorical_7  \\\n",
              "711        0.319769       0.190861       0.142159       0.393587   \n",
              "475       -0.283131       0.046135       0.316052      -0.120822   \n",
              "273        0.090909       0.003120       1.558923       0.244971   \n",
              "9         -0.252932       0.293856      -0.197504       0.050085   \n",
              "275        0.090909      -0.760367       0.326620      -0.037488   \n",
              "336       -0.532591       0.338287       0.071977       0.113531   \n",
              "1462      -0.283131      -0.270112      -0.197504       0.068821   \n",
              "1436       0.338006       0.479294       0.063384       0.342170   \n",
              "1611       0.212399      -1.811086       0.204642      -0.622682   \n",
              "166        0.323870       0.279502      -0.533179       0.050085   \n",
              "\n",
              "      categorical_8  categorical_9  \n",
              "711       -0.766454      -0.312227  \n",
              "475       -0.425927      -0.163454  \n",
              "273       -0.387846      -0.327537  \n",
              "9         -0.587633      -0.413439  \n",
              "275       -0.121713      -0.244310  \n",
              "336        0.527567       1.290724  \n",
              "1462       0.371461       0.966579  \n",
              "1436      -0.054090      -0.163454  \n",
              "1611      -0.425927      -0.489371  \n",
              "166        0.016449       1.410449  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "87YTp1sEz5PP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Note that the size of our target-encoded matrix is the same size as the original (unlike the huge one-hot transformed matrix):"
      ]
    },
    {
      "metadata": {
        "id": "thHOhchhz8mk",
        "colab_type": "code",
        "outputId": "3276a603-6b79-43d8-9c0a-7b92cdcbee5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# Compare sizes\n",
        "print('Original size:', X_train.shape)\n",
        "print('Target encoded size:', X_target_encoded.shape)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original size: (1000, 10)\n",
            "Target encoded size: (1000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SRgdJww0Zj0N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Also, each column has exactly as many unique continuous values as it did categories.  This is because we've simply replaced the category with the mean target value for that category."
      ]
    },
    {
      "metadata": {
        "id": "QpKHqIysZvcm",
        "colab_type": "code",
        "outputId": "25976035-9399-4f20-cd0c-db32df99864c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "cell_type": "code",
      "source": [
        "# Compare category counts\n",
        "print('Original:')\n",
        "print(X_train.nunique())\n",
        "print('\\nTarget encoded:')\n",
        "print(X_target_encoded.nunique())"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:\n",
            "categorical_0    84\n",
            "categorical_1    81\n",
            "categorical_2    85\n",
            "categorical_3    88\n",
            "categorical_4    84\n",
            "categorical_5    86\n",
            "categorical_6    88\n",
            "categorical_7    88\n",
            "categorical_8    90\n",
            "categorical_9    79\n",
            "dtype: int64\n",
            "\n",
            "Target encoded:\n",
            "categorical_0    84\n",
            "categorical_1    81\n",
            "categorical_2    85\n",
            "categorical_3    88\n",
            "categorical_4    84\n",
            "categorical_5    86\n",
            "categorical_6    88\n",
            "categorical_7    88\n",
            "categorical_8    90\n",
            "categorical_9    79\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iTTtxLxnWEN8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If we fit the same model as before, but now after target-encoding the categories, our predictions get much better!"
      ]
    },
    {
      "metadata": {
        "id": "q7j8DFVNWGiF",
        "colab_type": "code",
        "outputId": "918708cd-63cf-45fc-d657-fcdcff2e2958",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Regression model\n",
        "model_te = Pipeline([\n",
        "    ('encoder', TargetEncoder()),\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('regressor', BayesianRidge())\n",
        "])\n",
        "\n",
        "# Cross-validated MAE\n",
        "scores = cross_val_score(model_te, X_train, y_train, \n",
        "                         cv=3, scoring=mae_scorer)\n",
        "print('Cross-validated MAE:', scores.mean(), '+/-', scores.std())"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cross-validated MAE: 0.9402165006761526 +/- 0.029713106940791913\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RA5txfaXcdcA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The performance on the test data is about the same, but slightly better, because we've given it more samples on which to train.\n"
      ]
    },
    {
      "metadata": {
        "id": "Sq3DV6aIYMeE",
        "colab_type": "code",
        "outputId": "3d6f9681-36ba-47f3-dd2e-1f61bdf14923",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# MAE on test data\n",
        "model_te.fit(X_train, y_train)\n",
        "y_pred = model_te.predict(X_test)\n",
        "test_mae = mean_absolute_error(y_test, y_pred)\n",
        "print('Validation MAE:', test_mae)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation MAE: 0.9325682408250325\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lTsmSMPyYPtV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "While the error is lower using target encoding than with one-hot encoding, in naively target-encoding our categories, we've introduced a data leak from the target variable for one sample into the features for that same sample!  \n",
        "\n",
        "TODO: explain w/ diagrams\n",
        "\n",
        "Leaking the target variable into our predictors causes our learning algorithm to over-depend on the target-encoded features, and overfit the data.  Although we gain predictive power by keeping the dimensionality of our training data reasonable, we loose nearly all of that gain by allowing our model to overfit to the target-encoded columns!"
      ]
    },
    {
      "metadata": {
        "id": "d_CvINwf2v9f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Cross-Fold Target Encoding\n",
        "\n",
        "To clamp down on the data leakage, we need to ensure that we're not using the using the target value from a given sample to compute its target-encoded values.  However, we can still use *other* samples in the training data to compute the mean target values for *this* sample's category.  \n",
        "\n",
        "There are a few different ways we can do this.  We could compute the per-category target means in a cross-fold fashion, or by leaving current sample out (leave-one-out).\n",
        "\n",
        "TODO: explain cross-fold w /diagrams"
      ]
    },
    {
      "metadata": {
        "id": "lls4RKG-2vZ4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class TargetEncoderCV(TargetEncoder):\n",
        "    \"\"\"Cross-validated target encoder.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, n_splits=3, shuffle=True, cols=None):\n",
        "        \"\"\"Cross-validated target encoding for categorical features.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        n_splits : int\n",
        "            Number of cross-validation splits. Default = 3.\n",
        "        shuffle : bool\n",
        "            Whether to shuffle the data when splitting into folds.\n",
        "        cols : list of str\n",
        "            Columns to target encode.\n",
        "        \"\"\"\n",
        "        self.n_splits = n_splits\n",
        "        self.shuffle = shuffle\n",
        "        self.cols = cols\n",
        "        \n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Fit target encoder to X and y\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        X : pandas DataFrame, shape [n_samples, n_columns]\n",
        "            DataFrame containing columns to encode\n",
        "        y : pandas Series, shape = [n_samples]\n",
        "            Target values.\n",
        "            \n",
        "        Returns\n",
        "        -------\n",
        "        self : encoder\n",
        "            Returns self.\n",
        "        \"\"\"\n",
        "        self._target_encoder = TargetEncoder(cols=self.cols)\n",
        "        self._target_encoder.fit(X, y)\n",
        "        return self\n",
        "\n",
        "    \n",
        "    def transform(self, X, y=None):\n",
        "        \"\"\"Perform the target encoding transformation.\n",
        "\n",
        "        Uses cross-validated target encoding for the training fold, and uses\n",
        "        normal target encoding for the test fold.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : pandas DataFrame, shape [n_samples, n_columns]\n",
        "            DataFrame containing columns to encode\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        pandas DataFrame\n",
        "            Input DataFrame with transformed columns\n",
        "        \"\"\"\n",
        "\n",
        "        # Use target encoding from fit() if this is test data\n",
        "        if y is None:\n",
        "            return self._target_encoder.transform(X)\n",
        "\n",
        "        # Compute means for each fold\n",
        "        self._train_ix = []\n",
        "        self._test_ix = []\n",
        "        self._fit_tes = []\n",
        "        kf = KFold(n_splits=self.n_splits, shuffle=self.shuffle)\n",
        "        for train_ix, test_ix in kf.split(X):\n",
        "            self._train_ix.append(train_ix)\n",
        "            self._test_ix.append(test_ix)\n",
        "            te = TargetEncoder(cols=self.cols)\n",
        "            if isinstance(X, pd.DataFrame):\n",
        "                self._fit_tes.append(te.fit(X.iloc[train_ix,:],\n",
        "                                            y.iloc[train_ix]))\n",
        "            elif isinstance(X, np.ndarray):\n",
        "                self._fit_tes.append(te.fit(X[train_ix,:], y[train_ix]))\n",
        "            else:\n",
        "                raise TypeError('X must be DataFrame or ndarray')\n",
        "\n",
        "        # Apply means across folds\n",
        "        Xo = X.copy()\n",
        "        for ix in range(len(self._test_ix)):\n",
        "            test_ix = self._test_ix[ix]\n",
        "            if isinstance(X, pd.DataFrame):\n",
        "                Xo.iloc[test_ix,:] = self._fit_tes[ix].transform(X.iloc[test_ix,:])\n",
        "            elif isinstance(X, np.ndarray):\n",
        "                Xo[test_ix,:] = self._fit_tes[ix].transform(X[test_ix,:])\n",
        "            else:\n",
        "                raise TypeError('X must be DataFrame or ndarray')\n",
        "        return Xo\n",
        "\n",
        "            \n",
        "    def fit_transform(self, X, y=None):\n",
        "        \"\"\"Fit and transform the data via target encoding.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        X : pandas DataFrame, shape [n_samples, n_columns]\n",
        "            DataFrame containing columns to encode\n",
        "        y : pandas Series, shape = [n_samples]\n",
        "            Target values (required!).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        pandas DataFrame\n",
        "            Input DataFrame with transformed columns\n",
        "        \"\"\"\n",
        "        return self.fit(X, y).transform(X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2e3eQ8-6Z1It",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "With this encoder, we can convert the categories into continuous values, just like we did with the naive target encoding."
      ]
    },
    {
      "metadata": {
        "id": "FDlvSOS5Z12A",
        "colab_type": "code",
        "outputId": "5796be05-a562-4f45-f7b2-23ef3ee06fd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        }
      },
      "cell_type": "code",
      "source": [
        "# Cross-fold Target encode the categorical data\n",
        "te = TargetEncoderCV()\n",
        "X_target_encoded_cv = te.fit_transform(X_train, y_train)\n",
        "X_target_encoded_cv.sample(10)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>categorical_0</th>\n",
              "      <th>categorical_1</th>\n",
              "      <th>categorical_2</th>\n",
              "      <th>categorical_3</th>\n",
              "      <th>categorical_4</th>\n",
              "      <th>categorical_5</th>\n",
              "      <th>categorical_6</th>\n",
              "      <th>categorical_7</th>\n",
              "      <th>categorical_8</th>\n",
              "      <th>categorical_9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>236</th>\n",
              "      <td>0.233017</td>\n",
              "      <td>0.266851</td>\n",
              "      <td>0.620411</td>\n",
              "      <td>-0.0917691</td>\n",
              "      <td>0.0238002</td>\n",
              "      <td>0.205387</td>\n",
              "      <td>-0.182844</td>\n",
              "      <td>0.209843</td>\n",
              "      <td>-0.201101</td>\n",
              "      <td>1.95302</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>0.0419825</td>\n",
              "      <td>-0.133423</td>\n",
              "      <td>-0.152355</td>\n",
              "      <td>0.768532</td>\n",
              "      <td>-0.105238</td>\n",
              "      <td>-0.0010254</td>\n",
              "      <td>-0.0768051</td>\n",
              "      <td>-0.164632</td>\n",
              "      <td>-0.108223</td>\n",
              "      <td>-1.16817</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>502</th>\n",
              "      <td>0.335987</td>\n",
              "      <td>0.686906</td>\n",
              "      <td>0.00831367</td>\n",
              "      <td>0.780618</td>\n",
              "      <td>0.253249</td>\n",
              "      <td>-0.588782</td>\n",
              "      <td>-0.0104415</td>\n",
              "      <td>-0.139042</td>\n",
              "      <td>0.258339</td>\n",
              "      <td>2.08561</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>870</th>\n",
              "      <td>0.251286</td>\n",
              "      <td>-0.221214</td>\n",
              "      <td>-0.21522</td>\n",
              "      <td>-0.528595</td>\n",
              "      <td>-0.320334</td>\n",
              "      <td>0.484078</td>\n",
              "      <td>0.593479</td>\n",
              "      <td>0.131563</td>\n",
              "      <td>0.152882</td>\n",
              "      <td>-0.333682</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>357</th>\n",
              "      <td>-0.105762</td>\n",
              "      <td>-0.108175</td>\n",
              "      <td>-0.2422</td>\n",
              "      <td>-1.08681</td>\n",
              "      <td>-0.0790136</td>\n",
              "      <td>-0.367782</td>\n",
              "      <td>0.287205</td>\n",
              "      <td>0.542695</td>\n",
              "      <td>0.064133</td>\n",
              "      <td>-0.670692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1372</th>\n",
              "      <td>0.169969</td>\n",
              "      <td>0.366924</td>\n",
              "      <td>0.399639</td>\n",
              "      <td>-0.0954622</td>\n",
              "      <td>0.0220233</td>\n",
              "      <td>-0.588782</td>\n",
              "      <td>-0.529951</td>\n",
              "      <td>0.233605</td>\n",
              "      <td>-0.260713</td>\n",
              "      <td>-0.130225</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>620</th>\n",
              "      <td>0.372039</td>\n",
              "      <td>0.110516</td>\n",
              "      <td>-0.259249</td>\n",
              "      <td>-0.0814691</td>\n",
              "      <td>0.294292</td>\n",
              "      <td>0.705151</td>\n",
              "      <td>0.300228</td>\n",
              "      <td>0.227451</td>\n",
              "      <td>0.185972</td>\n",
              "      <td>1.53523</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1147</th>\n",
              "      <td>-0.294882</td>\n",
              "      <td>0.477974</td>\n",
              "      <td>0.531971</td>\n",
              "      <td>0.210054</td>\n",
              "      <td>-0.171589</td>\n",
              "      <td>-0.106227</td>\n",
              "      <td>0.0837924</td>\n",
              "      <td>-0.201896</td>\n",
              "      <td>-0.595051</td>\n",
              "      <td>0.659421</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1650</th>\n",
              "      <td>-0.882803</td>\n",
              "      <td>0.647945</td>\n",
              "      <td>0.177125</td>\n",
              "      <td>-0.190479</td>\n",
              "      <td>0.644579</td>\n",
              "      <td>0.208487</td>\n",
              "      <td>0.657135</td>\n",
              "      <td>0.227451</td>\n",
              "      <td>-0.701029</td>\n",
              "      <td>-0.00746989</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.831874</td>\n",
              "      <td>-0.113836</td>\n",
              "      <td>-0.190479</td>\n",
              "      <td>-0.475449</td>\n",
              "      <td>-1.90497</td>\n",
              "      <td>-0.991536</td>\n",
              "      <td>0.649424</td>\n",
              "      <td>-0.326514</td>\n",
              "      <td>-0.2205</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     categorical_0 categorical_1 categorical_2 categorical_3 categorical_4  \\\n",
              "236       0.233017      0.266851      0.620411    -0.0917691     0.0238002   \n",
              "103      0.0419825     -0.133423     -0.152355      0.768532     -0.105238   \n",
              "502       0.335987      0.686906    0.00831367      0.780618      0.253249   \n",
              "870       0.251286     -0.221214      -0.21522     -0.528595     -0.320334   \n",
              "357      -0.105762     -0.108175       -0.2422      -1.08681    -0.0790136   \n",
              "1372      0.169969      0.366924      0.399639    -0.0954622     0.0220233   \n",
              "620       0.372039      0.110516     -0.259249    -0.0814691      0.294292   \n",
              "1147     -0.294882      0.477974      0.531971      0.210054     -0.171589   \n",
              "1650     -0.882803      0.647945      0.177125     -0.190479      0.644579   \n",
              "68             NaN      0.831874     -0.113836     -0.190479     -0.475449   \n",
              "\n",
              "     categorical_5 categorical_6 categorical_7 categorical_8 categorical_9  \n",
              "236       0.205387     -0.182844      0.209843     -0.201101       1.95302  \n",
              "103     -0.0010254    -0.0768051     -0.164632     -0.108223      -1.16817  \n",
              "502      -0.588782    -0.0104415     -0.139042      0.258339       2.08561  \n",
              "870       0.484078      0.593479      0.131563      0.152882     -0.333682  \n",
              "357      -0.367782      0.287205      0.542695      0.064133     -0.670692  \n",
              "1372     -0.588782     -0.529951      0.233605     -0.260713     -0.130225  \n",
              "620       0.705151      0.300228      0.227451      0.185972       1.53523  \n",
              "1147     -0.106227     0.0837924     -0.201896     -0.595051      0.659421  \n",
              "1650      0.208487      0.657135      0.227451     -0.701029   -0.00746989  \n",
              "68        -1.90497     -0.991536      0.649424     -0.326514       -0.2205  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "metadata": {
        "id": "nLgqJ2xo0uxQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Like with normal target encoding, our transformed matrix is the same shape as the original:"
      ]
    },
    {
      "metadata": {
        "id": "UcVvL91V00HD",
        "colab_type": "code",
        "outputId": "ab915ce5-80a1-4cd0-f02a-c1c7cc4e7c97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# Compare sizes\n",
        "print('Original size:', X_train.shape)\n",
        "print('Target encoded size:', X_target_encoded_cv.shape)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original size: (1000, 10)\n",
            "Target encoded size: (1000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uZrZLdx-Z81V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "However, now we have more unique continuous values in each column than we did categories, because we've target-encoded the categories separately for each fold."
      ]
    },
    {
      "metadata": {
        "id": "vuqXaUqIaEXl",
        "colab_type": "code",
        "outputId": "8741cec2-1085-4326-cd56-52535473f6a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "cell_type": "code",
      "source": [
        "# Compare category counts\n",
        "print('Original:')\n",
        "print(X_train.nunique())\n",
        "print('\\nTarget encoded:')\n",
        "print(X_target_encoded_cv.nunique())"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:\n",
            "categorical_0    84\n",
            "categorical_1    81\n",
            "categorical_2    85\n",
            "categorical_3    88\n",
            "categorical_4    84\n",
            "categorical_5    86\n",
            "categorical_6    88\n",
            "categorical_7    88\n",
            "categorical_8    90\n",
            "categorical_9    79\n",
            "dtype: int64\n",
            "\n",
            "Target encoded:\n",
            "categorical_0    214\n",
            "categorical_1    203\n",
            "categorical_2    201\n",
            "categorical_3    203\n",
            "categorical_4    208\n",
            "categorical_5    207\n",
            "categorical_6    207\n",
            "categorical_7    205\n",
            "categorical_8    213\n",
            "categorical_9    200\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "I1dys8eHxYAq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "TODO: then we can fit the same model as before, but w/ cross-fold target encoding..."
      ]
    },
    {
      "metadata": {
        "id": "lOn6xCnqxZ3T",
        "colab_type": "code",
        "outputId": "3ba31992-0012-4b74-d26d-6a5bc19cd9d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Regression model\n",
        "model_te_cv = Pipeline([\n",
        "    ('encoder', TargetEncoderCV()),\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('regressor', BayesianRidge())\n",
        "])\n",
        "\n",
        "# Cross-validated MAE\n",
        "scores = cross_val_score(model_te_cv, X_train, y_train, \n",
        "                         cv=3, scoring=mae_scorer)\n",
        "print('Cross-validated MAE:', scores.mean(), '+/-', scores.std())"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cross-validated MAE: 0.8351049783190474 +/- 0.04365424683197591\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GCjhesDoxtfK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "TODO: and now the validation performance is consistent with the cross-validated performance"
      ]
    },
    {
      "metadata": {
        "id": "uIj56K0Hx8A7",
        "colab_type": "code",
        "outputId": "f02c0bc7-5c0a-4b80-a5bc-5fa96d3e5dfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# MAE on test data\n",
        "model_te_cv.fit(X_train, y_train)\n",
        "y_pred = model_te_cv.predict(X_test)\n",
        "test_mae = mean_absolute_error(y_test, y_pred)\n",
        "print('Validation MAE:', test_mae)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation MAE: 0.8389909316238072\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WAyX99su6Ztp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Leave-one-out Target Encoding\n",
        "\n",
        "We could also prevent the target data leakage by using a leave-one-out scheme.  With this method, we compute the per-category means as before, but we don't include the current sample in the computation.\n",
        "\n",
        "TODO: explain w/ diagrams"
      ]
    },
    {
      "metadata": {
        "id": "XXc0S-_m68qY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class TargetEncoderLOO(TargetEncoder):\n",
        "    \"\"\"Leave-one-out target encoder.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, n_splits=3, shuffle=True, cols=None):\n",
        "        \"\"\"Leave-one-out target encoding for categorical features.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        cols : list of str\n",
        "            Columns to target encode.\n",
        "        \"\"\"\n",
        "        self.cols = cols\n",
        "        \n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Fit leave-one-out target encoder to X and y\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        X : pandas DataFrame, shape [n_samples, n_columns]\n",
        "            DataFrame containing columns to target encode\n",
        "        y : pandas Series, shape = [n_samples]\n",
        "            Target values.\n",
        "            \n",
        "        Returns\n",
        "        -------\n",
        "        self : encoder\n",
        "            Returns self.\n",
        "        \"\"\"\n",
        "        \n",
        "        # Encode all categorical cols by default\n",
        "        if self.cols is None:\n",
        "            self.cols = [col for col in X if str(X[col].dtype)=='object']\n",
        "\n",
        "        # Check columns are in X\n",
        "        for col in self.cols:\n",
        "            if col not in X:\n",
        "                raise ValueError('Column \\''+col+'\\' not in X')\n",
        "\n",
        "        # Encode each element of each column\n",
        "        self.sum_count = dict() #dict for sum + counts for each column\n",
        "        for col in self.cols:\n",
        "            self.sum_count[col] = dict()\n",
        "            uniques = X[col].unique()\n",
        "            for unique in uniques:\n",
        "                ix = X[col]==unique\n",
        "                self.sum_count[col][unique] = (y[ix].sum(), ix.sum())\n",
        "            \n",
        "        # Return the fit object\n",
        "        return self\n",
        "\n",
        "    \n",
        "    def transform(self, X, y=None):\n",
        "        \"\"\"Perform the target encoding transformation.\n",
        "\n",
        "        Uses leave-one-out target encoding for the training fold, and\n",
        "        uses normal target encoding for the test fold.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : pandas DataFrame, shape [n_samples, n_columns]\n",
        "            DataFrame containing columns to encode\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        pandas DataFrame\n",
        "            Input DataFrame with transformed columns\n",
        "        \"\"\"\n",
        "        \n",
        "        # Create output dataframe\n",
        "        Xo = X.copy()\n",
        "\n",
        "        # Use normal target encoding if this is test data\n",
        "        if y is None:\n",
        "            for col in self.sum_count:\n",
        "                vals = np.full(X.shape[0], np.nan)\n",
        "                for cat, sum_count in self.sum_count[col].items():\n",
        "                    vals[X[col]==cat] = sum_count[0]/sum_count[1]\n",
        "                Xo[col] = vals\n",
        "\n",
        "        # LOO target encode each column\n",
        "        else:\n",
        "            for col in self.sum_count:\n",
        "                vals = np.full(X.shape[0], np.nan)\n",
        "                for cat, sum_count in self.sum_count[col].items():\n",
        "                    ix = X[col]==cat\n",
        "                    vals[ix] = (sum_count[0]-y[ix])/(sum_count[1]-1)\n",
        "                Xo[col] = vals\n",
        "            \n",
        "        # Return encoded DataFrame\n",
        "        return Xo\n",
        "      \n",
        "            \n",
        "    def fit_transform(self, X, y=None):\n",
        "        \"\"\"Fit and transform the data via target encoding.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        X : pandas DataFrame, shape [n_samples, n_columns]\n",
        "            DataFrame containing columns to encode\n",
        "        y : pandas Series, shape = [n_samples]\n",
        "            Target values (required!).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        pandas DataFrame\n",
        "            Input DataFrame with transformed columns\n",
        "        \"\"\"\n",
        "        return self.fit(X, y).transform(X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qJhMNfjbxs5u",
        "colab_type": "code",
        "outputId": "4635086b-9abe-453d-b9f6-6b58d3b2dd0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        }
      },
      "cell_type": "code",
      "source": [
        "# Cross-fold Target encode the categorical data\n",
        "te = TargetEncoderLOO()\n",
        "X_target_encoded_loo = te.fit_transform(X_train, y_train)\n",
        "X_target_encoded_loo.sample(10)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>categorical_0</th>\n",
              "      <th>categorical_1</th>\n",
              "      <th>categorical_2</th>\n",
              "      <th>categorical_3</th>\n",
              "      <th>categorical_4</th>\n",
              "      <th>categorical_5</th>\n",
              "      <th>categorical_6</th>\n",
              "      <th>categorical_7</th>\n",
              "      <th>categorical_8</th>\n",
              "      <th>categorical_9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>915</th>\n",
              "      <td>-0.012727</td>\n",
              "      <td>0.314121</td>\n",
              "      <td>-0.233903</td>\n",
              "      <td>0.175900</td>\n",
              "      <td>0.132491</td>\n",
              "      <td>-1.140074</td>\n",
              "      <td>0.069677</td>\n",
              "      <td>0.256593</td>\n",
              "      <td>-0.430652</td>\n",
              "      <td>-0.103827</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1647</th>\n",
              "      <td>0.215025</td>\n",
              "      <td>0.394832</td>\n",
              "      <td>-0.449700</td>\n",
              "      <td>0.600029</td>\n",
              "      <td>-0.201291</td>\n",
              "      <td>0.260293</td>\n",
              "      <td>0.191454</td>\n",
              "      <td>-2.622372</td>\n",
              "      <td>0.165401</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82</th>\n",
              "      <td>-0.025525</td>\n",
              "      <td>-0.097241</td>\n",
              "      <td>0.306800</td>\n",
              "      <td>0.302967</td>\n",
              "      <td>0.211843</td>\n",
              "      <td>-0.075201</td>\n",
              "      <td>0.487799</td>\n",
              "      <td>-0.062378</td>\n",
              "      <td>0.137296</td>\n",
              "      <td>-0.608199</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>328</th>\n",
              "      <td>-0.022190</td>\n",
              "      <td>0.183942</td>\n",
              "      <td>0.237844</td>\n",
              "      <td>-0.622328</td>\n",
              "      <td>-0.305189</td>\n",
              "      <td>0.444699</td>\n",
              "      <td>0.145842</td>\n",
              "      <td>0.451886</td>\n",
              "      <td>0.397333</td>\n",
              "      <td>0.897512</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>369</th>\n",
              "      <td>0.612622</td>\n",
              "      <td>-0.219190</td>\n",
              "      <td>0.430788</td>\n",
              "      <td>0.595137</td>\n",
              "      <td>-0.299720</td>\n",
              "      <td>-0.210595</td>\n",
              "      <td>-0.073953</td>\n",
              "      <td>-0.153809</td>\n",
              "      <td>0.342509</td>\n",
              "      <td>0.827628</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1488</th>\n",
              "      <td>0.158465</td>\n",
              "      <td>-0.104834</td>\n",
              "      <td>0.117011</td>\n",
              "      <td>1.055839</td>\n",
              "      <td>0.071813</td>\n",
              "      <td>-0.166491</td>\n",
              "      <td>-0.598097</td>\n",
              "      <td>0.394334</td>\n",
              "      <td>-0.909585</td>\n",
              "      <td>-0.650951</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>800</th>\n",
              "      <td>0.287101</td>\n",
              "      <td>-0.072489</td>\n",
              "      <td>0.402740</td>\n",
              "      <td>-0.044492</td>\n",
              "      <td>0.157021</td>\n",
              "      <td>0.542464</td>\n",
              "      <td>0.601372</td>\n",
              "      <td>0.215462</td>\n",
              "      <td>0.184972</td>\n",
              "      <td>-2.275490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>735</th>\n",
              "      <td>0.050074</td>\n",
              "      <td>0.252479</td>\n",
              "      <td>-0.164370</td>\n",
              "      <td>-0.143356</td>\n",
              "      <td>0.126941</td>\n",
              "      <td>0.417575</td>\n",
              "      <td>0.044603</td>\n",
              "      <td>-0.167643</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.427960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1361</th>\n",
              "      <td>0.572234</td>\n",
              "      <td>0.578534</td>\n",
              "      <td>-0.286133</td>\n",
              "      <td>0.142657</td>\n",
              "      <td>-0.537536</td>\n",
              "      <td>-0.144131</td>\n",
              "      <td>1.009878</td>\n",
              "      <td>0.019303</td>\n",
              "      <td>-0.524030</td>\n",
              "      <td>0.880222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1355</th>\n",
              "      <td>-0.188658</td>\n",
              "      <td>-0.107702</td>\n",
              "      <td>0.052277</td>\n",
              "      <td>-0.121903</td>\n",
              "      <td>0.130991</td>\n",
              "      <td>-0.264774</td>\n",
              "      <td>0.082149</td>\n",
              "      <td>0.031939</td>\n",
              "      <td>0.136631</td>\n",
              "      <td>0.335117</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      categorical_0  categorical_1  categorical_2  categorical_3  \\\n",
              "915       -0.012727       0.314121      -0.233903       0.175900   \n",
              "1647       0.215025       0.394832      -0.449700       0.600029   \n",
              "82        -0.025525      -0.097241       0.306800       0.302967   \n",
              "328       -0.022190       0.183942       0.237844      -0.622328   \n",
              "369        0.612622      -0.219190       0.430788       0.595137   \n",
              "1488       0.158465      -0.104834       0.117011       1.055839   \n",
              "800        0.287101      -0.072489       0.402740      -0.044492   \n",
              "735        0.050074       0.252479      -0.164370      -0.143356   \n",
              "1361       0.572234       0.578534      -0.286133       0.142657   \n",
              "1355      -0.188658      -0.107702       0.052277      -0.121903   \n",
              "\n",
              "      categorical_4  categorical_5  categorical_6  categorical_7  \\\n",
              "915        0.132491      -1.140074       0.069677       0.256593   \n",
              "1647      -0.201291       0.260293       0.191454      -2.622372   \n",
              "82         0.211843      -0.075201       0.487799      -0.062378   \n",
              "328       -0.305189       0.444699       0.145842       0.451886   \n",
              "369       -0.299720      -0.210595      -0.073953      -0.153809   \n",
              "1488       0.071813      -0.166491      -0.598097       0.394334   \n",
              "800        0.157021       0.542464       0.601372       0.215462   \n",
              "735        0.126941       0.417575       0.044603      -0.167643   \n",
              "1361      -0.537536      -0.144131       1.009878       0.019303   \n",
              "1355       0.130991      -0.264774       0.082149       0.031939   \n",
              "\n",
              "      categorical_8  categorical_9  \n",
              "915       -0.430652      -0.103827  \n",
              "1647       0.165401            NaN  \n",
              "82         0.137296      -0.608199  \n",
              "328        0.397333       0.897512  \n",
              "369        0.342509       0.827628  \n",
              "1488      -0.909585      -0.650951  \n",
              "800        0.184972      -2.275490  \n",
              "735             NaN      -0.427960  \n",
              "1361      -0.524030       0.880222  \n",
              "1355       0.136631       0.335117  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "metadata": {
        "id": "q7NxW09g1Lgr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The transformed matrix is stil the same size as the original:"
      ]
    },
    {
      "metadata": {
        "id": "wE690mm41Pfn",
        "colab_type": "code",
        "outputId": "078f1d66-479a-4229-dfe8-572585a0779e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# Compare sizes\n",
        "print('Original size:', X_train.shape)\n",
        "print('Target encoded size:', X_target_encoded_loo.shape)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original size: (1000, 10)\n",
            "Target encoded size: (1000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hFg2GYU61cde",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "But now there are nearly as many unique values in each column as there are samples:"
      ]
    },
    {
      "metadata": {
        "id": "HEQPxiJK1QCU",
        "colab_type": "code",
        "outputId": "77edded5-50d4-449d-a9e4-756aee4ecc09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "cell_type": "code",
      "source": [
        "# Compare category counts\n",
        "print('Original:')\n",
        "print(X_train.nunique())\n",
        "print('\\nLeave-one-out target encoded:')\n",
        "print(X_target_encoded_loo.nunique())"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:\n",
            "categorical_0    84\n",
            "categorical_1    81\n",
            "categorical_2    85\n",
            "categorical_3    88\n",
            "categorical_4    84\n",
            "categorical_5    86\n",
            "categorical_6    88\n",
            "categorical_7    88\n",
            "categorical_8    90\n",
            "categorical_9    79\n",
            "dtype: int64\n",
            "\n",
            "Leave-one-out target encoded:\n",
            "categorical_0    993\n",
            "categorical_1    994\n",
            "categorical_2    992\n",
            "categorical_3    987\n",
            "categorical_4    990\n",
            "categorical_5    990\n",
            "categorical_6    990\n",
            "categorical_7    991\n",
            "categorical_8    992\n",
            "categorical_9    996\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "x8E8OZ5y1oNy",
        "colab_type": "code",
        "outputId": "2d1778f9-88a0-42e5-e157-3530fb753995",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Regression model\n",
        "model_te_loo = Pipeline([\n",
        "    ('encoder', TargetEncoderLOO()),\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('regressor', BayesianRidge())\n",
        "])\n",
        "\n",
        "# Cross-validated MAE\n",
        "scores = cross_val_score(model_te_loo, X_train, y_train, \n",
        "                         cv=3, scoring=mae_scorer)\n",
        "print('Cross-validated MAE:', scores.mean(), '+/-', scores.std())"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cross-validated MAE: 0.8329997501949703 +/- 0.037514587474735996\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aT3SSVXu1xkB",
        "colab_type": "code",
        "outputId": "faa111b1-efa9-42f6-fb1d-af5ba0f333a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# MAE on test data\n",
        "model_te_loo.fit(X_train, y_train)\n",
        "y_pred = model_te_loo.predict(X_test)\n",
        "test_mae = mean_absolute_error(y_test, y_pred)\n",
        "print('Validation MAE:', test_mae)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation MAE: 0.837977841124484\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8Mm-T-4_2yvx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The leave-one-out target encoder performs *slightly* better than the cross-fold target encoder, because we've given it more samples with which to compute the per-category means ($N-1$, instead of $N-N/K$, where K is the number of folds).  While the increase in performance was very small, the leave-one-out target encoder is faster, due to the effecient way we computed the leave-one-out means (instead of having to compute means for each fold)."
      ]
    },
    {
      "metadata": {
        "id": "HooowJsY2uLm",
        "colab_type": "code",
        "outputId": "b0a3c1bb-e127-4125-e6e5-49ace3766835",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "Xo = TargetEncoderCV().fit_transform(X_train, y_train)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 6.81 s, sys: 105 ms, total: 6.92 s\n",
            "Wall time: 6.83 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lOW06X6L2c5M",
        "colab_type": "code",
        "outputId": "053fd6f1-d6c5-454f-f370-0bd655bc85e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "Xo = TargetEncoderLOO().fit_transform(X_train, y_train)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 4.23 s, sys: 17.1 ms, total: 4.25 s\n",
            "Wall time: 4.25 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_zSd5DXn16YA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Effect of the Learning Algorithm\n",
        "\n",
        "The increase in predictive performance one gets from target encoding depends on the machine learning algorithm which is using it.  As we've seen, target encoding is great for linear models (throughout this post we were using a Bayesian ridge regression, a variant on a linear regression which optimizes the regularization parameter).  However, target encoding doesn't help as much for tree-based boosting algorithms like XGBoost, CatBoost, or LightGBM, which tend to handle categorical data pretty well as-is.\n",
        "\n",
        "Fitting the Bayesian ridge regression to the data, we see a huge increase in performance after target encoding (relative to one-hot encoding)."
      ]
    },
    {
      "metadata": {
        "id": "gkUC7smw7HKF",
        "colab_type": "code",
        "outputId": "38d3a02b-930a-4af0-b797-c4238e0a1de7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Bayesian ridge w/ one-hot encoding\n",
        "model_brr = Pipeline([\n",
        "    ('encoder', OneHotEncoder()),\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('regressor', BayesianRidge())\n",
        "])\n",
        "\n",
        "# Cross-validated MAE\n",
        "scores = cross_val_score(model_brr, X_train, y_train, \n",
        "                         cv=3, scoring=mae_scorer)\n",
        "print('MAE w/ Bayesian Ridge + one-hot encoding: %0.3f +/- %0.3f' \n",
        "      % (scores.mean(), scores.std()))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MAE w/ Bayesian Ridge + one-hot encoding: 1.039 +/- 0.028\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vF8xDxC37d19",
        "colab_type": "code",
        "outputId": "d39d930c-524e-4a41-e347-fa4d3bea41b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Bayesian ridge w/ target-encoding\n",
        "model_brr = Pipeline([\n",
        "    ('encoder', TargetEncoderLOO()),\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('regressor', BayesianRidge())\n",
        "])\n",
        "\n",
        "# Cross-validated MAE\n",
        "scores = cross_val_score(model_brr, X_train, y_train, \n",
        "                         cv=3, scoring=mae_scorer)\n",
        "print('MAE w/ Bayesian Ridge + target encoding: %0.3f +/- %0.3f' \n",
        "      % (scores.mean(), scores.std()))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MAE w/ Bayesian Ridge + target encoding: 0.833 +/- 0.038\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "d9LXpbu06JKV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "However, using XGBoost, there is only a modest perfomance increase (if any at all)."
      ]
    },
    {
      "metadata": {
        "id": "13JgB55a6Bgh",
        "colab_type": "code",
        "outputId": "db6759db-e710-4820-c2e6-aaf2c1c60b0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Regression model\n",
        "model_xgb = Pipeline([\n",
        "    ('encoder', OneHotEncoder()),\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('regressor', XGBRegressor())\n",
        "])\n",
        "\n",
        "# Cross-validated MAE\n",
        "scores = cross_val_score(model_xgb, X_train, y_train, \n",
        "                         cv=3, scoring=mae_scorer)\n",
        "print('MAE w/ XGBoost + one-hot encoding: %0.3f +/- %0.3f'\n",
        "      % (scores.mean(), scores.std()))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MAE w/ XGBoost + one-hot encoding: 0.869 +/- 0.040\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8ctGdYDM6a_i",
        "colab_type": "code",
        "outputId": "cae1dc4a-4314-4ef3-b8a9-f39ce771940f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Regression model\n",
        "model_xgb = Pipeline([\n",
        "    ('encoder', TargetEncoderLOO()),\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('regressor', XGBRegressor())\n",
        "])\n",
        "\n",
        "# Cross-validated MAE\n",
        "scores = cross_val_score(model_xgb, X_train, y_train, \n",
        "                         cv=3, scoring=mae_scorer)\n",
        "print('MAE w/ XGBoost + target encoding: %0.3f +/- %0.3f'\n",
        "      % (scores.mean(), scores.std()))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MAE w/ XGBoost + target encoding: 0.864 +/- 0.052\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hNryKyVCM0cr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Dependence on the Number of Categories\n",
        "\n",
        "There is also an effect of the number of categories on the performance of a model trained on target-encoded data.  Target encoding works well with categorical data that contains a large number of categories.  However, if you have data with only a few categories, you're probably better off using one-hot encoding.\n",
        "\n",
        "For example, let's generate two datasets: one which has a large number of categories in each column, and another which has only a few categories in each column."
      ]
    },
    {
      "metadata": {
        "id": "nACpTFNGM-57",
        "colab_type": "code",
        "outputId": "ffca87e7-7070-44ec-eb51-323ae7bea9d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "# Categorical data w/ many categories\n",
        "X_many, y_many = make_categorical_regression(\n",
        "    n_samples=1000, \n",
        "    n_features=10, \n",
        "    n_categories=100,\n",
        "    n_informative=1,\n",
        "    imbalance=2.0)\n",
        "\n",
        "# Categorical data w/ few categories\n",
        "X_few, y_few = make_categorical_regression(\n",
        "    n_samples=1000, \n",
        "    n_features=10, \n",
        "    n_categories=5,\n",
        "    n_informative=1,\n",
        "    imbalance=2.0)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:72: DeprecationWarning: `comb` is deprecated!\n",
            "Importing `comb` from scipy.misc is deprecated in scipy 1.0.0. Use `scipy.special.comb` instead.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "KhbbmE3YBAF7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Then we'll construct two separate models: one which uses target-encoding, and another which uses one-hot encoding."
      ]
    },
    {
      "metadata": {
        "id": "jOUh-ko1-v24",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Regression model w/ target encoding\n",
        "model_te = Pipeline([\n",
        "    ('encoder', TargetEncoderLOO()),\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('regressor', BayesianRidge())\n",
        "])\n",
        "\n",
        "# Regression model w/ one-hot encoding\n",
        "model_oh = Pipeline([\n",
        "    ('encoder', OneHotEncoder()),\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('regressor', BayesianRidge())\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qU22JGVLBGxE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "On the dataset with many categories per column, target-encoding outperforms one-hot encoding by a good margin."
      ]
    },
    {
      "metadata": {
        "id": "edYl61Ji-ZuD",
        "colab_type": "code",
        "outputId": "723b49da-3f5c-4f23-a5f1-eafdd6ea065f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "print('Many categories:')\n",
        "\n",
        "# Target encoding w/ many categories\n",
        "scores = cross_val_score(model_te, X_many, y_many, \n",
        "                         cv=3, scoring=mae_scorer)\n",
        "print('MAE w/ target encoding: %0.3f +/- %0.3f' \n",
        "      % (scores.mean(), scores.std()))\n",
        "\n",
        "# One-hot encoding w/ many categories\n",
        "scores = cross_val_score(model_oh, X_many, y_many, \n",
        "                         cv=3, scoring=mae_scorer)\n",
        "print('MAE w/ one-hot encoding: %0.3f +/- %0.3f' \n",
        "      % (scores.mean(), scores.std()))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Many categories:\n",
            "MAE w/ target encoding: 0.820 +/- 0.029\n",
            "MAE w/ one-hot encoding: 1.049 +/- 0.045\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "i5LoSWmoBO-D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "On the other hand, with the dataset containing only a few categories per column, the performance of the one-hot encoded model is nearly indistinguishable from the performance of the model which uses target encoding."
      ]
    },
    {
      "metadata": {
        "id": "09nZjFL9_4zk",
        "colab_type": "code",
        "outputId": "1463d14d-c649-44cd-a162-03a61e4f6966",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "print('Few categories:')\n",
        "\n",
        "# Target encoding w/ few categories\n",
        "scores = cross_val_score(model_te, X_few, y_few, \n",
        "                         cv=3, scoring=mae_scorer)\n",
        "print('MAE w/ target encoding: %0.3f +/- %0.3f' \n",
        "      % (scores.mean(), scores.std()))\n",
        "\n",
        "# One-hot encoding w/ few categories\n",
        "scores = cross_val_score(model_oh, X_few, y_few, \n",
        "                         cv=3, scoring=mae_scorer)\n",
        "print('MAE w/ one-hot encoding: %0.3f +/- %0.3f' \n",
        "      % (scores.mean(), scores.std()))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Few categories:\n",
            "MAE w/ target encoding: 0.815 +/- 0.030\n",
            "MAE w/ one-hot encoding: 0.830 +/- 0.025\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zroBmNwL08_u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Effect of Category Imbalance\n",
        "\n",
        "I would have expected target encoding to perform better than one-hot encoding when the categories were extremely unbalanced (most samples have one of only a few categories), and one-hot encoding to outperform target encoding in the case of balanced categories (categories appear about the same number of times thoughout the dataset).  However, it appears that category imbalance effects both one-hot and target encoding similarly.  \n",
        "\n",
        "Let's generate two datasets, one of which has balanced categories, and another which has highly imbalanced categories in each column."
      ]
    },
    {
      "metadata": {
        "id": "qecVxs9g1Hdd",
        "colab_type": "code",
        "outputId": "e473af13-b058-4579-a973-6bb1644e34d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "# Categorical data w/ many categories\n",
        "X_bal, y_bal = make_categorical_regression(\n",
        "    n_samples=1000, \n",
        "    n_features=10, \n",
        "    n_categories=100,\n",
        "    n_informative=1,\n",
        "    imbalance=0.0)\n",
        "\n",
        "# Categorical data w/ few categories\n",
        "X_imbal, y_imbal = make_categorical_regression(\n",
        "    n_samples=1000, \n",
        "    n_features=10, \n",
        "    n_categories=100,\n",
        "    n_informative=1,\n",
        "    imbalance=2.0)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:72: DeprecationWarning: `comb` is deprecated!\n",
            "Importing `comb` from scipy.misc is deprecated in scipy 1.0.0. Use `scipy.special.comb` instead.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "Wdz8rzMYB3-y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Fitting the models from the previous section (one of which uses target encoding and the other uses one-hot encoding), we see that how imbalanced the data is doesn't have a huge effect on the perfomance of the model which uses target encoding."
      ]
    },
    {
      "metadata": {
        "id": "RZFLc7KcByWC",
        "colab_type": "code",
        "outputId": "c204e050-63cf-481a-d5b9-43c9e0e6c535",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "print('Target encoding:')\n",
        "\n",
        "# Target encoding w/ imbalanced categories\n",
        "scores = cross_val_score(model_te, X_imbal, y_imbal, \n",
        "                         cv=5, scoring=mae_scorer)\n",
        "print('MAE w/ imbalanced categories: %0.3f +/- %0.3f' \n",
        "      % (scores.mean(), scores.std()))\n",
        "\n",
        "# Target encoding w/ balanced categories\n",
        "scores = cross_val_score(model_te, X_bal, y_bal, \n",
        "                         cv=5, scoring=mae_scorer)\n",
        "print('MAE w/ balanced categories: %0.3f +/- %0.3f' \n",
        "      % (scores.mean(), scores.std()))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Target encoding:\n",
            "MAE w/ imbalanced categories: 0.873 +/- 0.054\n",
            "MAE w/ balanced categories: 0.845 +/- 0.041\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "l8qJyF0MJ8Lu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Nor does it appear to have a big effect on the performance of the model which uses one-hot encoding."
      ]
    },
    {
      "metadata": {
        "id": "v5y06BmOCCEn",
        "colab_type": "code",
        "outputId": "02b48230-828d-4be8-c544-c96aa4e92060",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "print('One-hot encoding:')\n",
        "\n",
        "# One-hot encoding w/ imbalanced categories\n",
        "scores = cross_val_score(model_oh, X_imbal, y_imbal, \n",
        "                         cv=5, scoring=mae_scorer)\n",
        "print('MAE w/ imbalanced categories: %0.3f +/- %0.3f' \n",
        "      % (scores.mean(), scores.std()))\n",
        "\n",
        "# One-hot encoding w/ balanced categories\n",
        "scores = cross_val_score(model_oh, X_bal, y_bal, \n",
        "                         cv=5, scoring=mae_scorer)\n",
        "print('MAE w/ balanced categories: %0.3f +/- %0.3f'\n",
        "      % (scores.mean(), scores.std()))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "One-hot encoding:\n",
            "MAE w/ imbalanced categories: 1.030 +/- 0.024\n",
            "MAE w/ balanced categories: 0.993 +/- 0.029\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "D47THvEXKWWg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "I've tried various combinations of predictive models, levels of imbalance, and numbers of categories, and the level of imbalance doesn't seem to have a very systematic effect.  I suspect this is because for both target encoding and one-hot encoding, with balanced categories we have more information about all categories on average (because examples with each category are more evenly distributed).  On the other hand, we have *less* information about the most common categories - because those categories are no more \"common\" than any other in a balanced dataset.  Therefore, the level of uncertainty for those categories ends up actually being higher for balanced datasets.  Those two effects appear to cancel out, and the predictive performance of our models don't change."
      ]
    },
    {
      "metadata": {
        "id": "pXAZQCjmNiYz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Effect of Interactions\n",
        "\n",
        "So far, target encoding has performed as well or better than other types of encoding.  However, there's one situation where target encoding doesn't do so well: in the face of strong interaction effects.\n",
        "\n",
        "An interaction effect is when the effect of one feature on the target variable depends on the value of a second feature.  For example, suppose we have one categorical feature with categories A and B, and a second categorical feature with categories C and D.  With no interaction effect, the effect of the first and second feature would be additive, and the effect of A and B on the target variable is independent of C and D.  An example of this is the money spent as a function of items purchased.  If a customer purchases both items 1 and 2, they will be charged the same as if they had purchased either item independently:"
      ]
    },
    {
      "metadata": {
        "id": "7lJbCN55mzSp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "outputId": "3cd9297b-f9ed-4466-af00-28e3498b14f6"
      },
      "cell_type": "code",
      "source": [
        "plt.bar(np.arange(4), [0, 2, 3, 5])\n",
        "plt.ylabel('Cost')\n",
        "plt.xticks(np.arange(4), \n",
        "           ['No purchases', \n",
        "            'Purchased only item 1', \n",
        "            'Purchased only item 2', \n",
        "            'Purchased both 1 + 2'])"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([<matplotlib.axis.XTick at 0x7f7ab683cc88>,\n",
              "  <matplotlib.axis.XTick at 0x7f7ab683c208>,\n",
              "  <matplotlib.axis.XTick at 0x7f7ab683c7f0>,\n",
              "  <matplotlib.axis.XTick at 0x7f7ab687fac8>],\n",
              " <a list of 4 Text xticklabel objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAFKCAYAAAAnj5dkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEGcAACxDnAEAsAxxBgDAMsQZAADL/B/PMD8pzLK2/gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "1rTCYLQeo1Tf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "On the other hand, if there is an interaction effect, the effect on the target variable will not be simply the sum of the two features' effects.  For example, just adding sugar *or* stirring coffee may not have a huge effect on the sweetness of the coffee.  But if one adds sugar *and* stirs, there is a large effect on the sweetness of the coffee."
      ]
    },
    {
      "metadata": {
        "id": "4F2kN9GAo1p_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "outputId": "08c95b9b-1365-45fe-fcda-59d7522c47c3"
      },
      "cell_type": "code",
      "source": [
        "plt.bar(np.arange(4), [1, 1, 3, 10])\n",
        "plt.ylabel('Coffee sweetness')\n",
        "plt.xticks(np.arange(4), \n",
        "           ['Nothing', \n",
        "            'Stir', \n",
        "            'Sugar', \n",
        "            'Sugar + stir'])"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([<matplotlib.axis.XTick at 0x7f7ab6651400>,\n",
              "  <matplotlib.axis.XTick at 0x7f7ab664b1d0>,\n",
              "  <matplotlib.axis.XTick at 0x7f7ab664bf98>,\n",
              "  <matplotlib.axis.XTick at 0x7f7ab6607400>],\n",
              " <a list of 4 Text xticklabel objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe0AAAFKCAYAAAAwrQetAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nA6VK89DMKjSJAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "O-VU_B5KmztO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "Target encoding simply fills in each category with the mean target value for samples having that category.  Because target encoding does this for each column individually, it's fundamentally unable to  handle interactions between columns!  That said, one-hot encoding doesn't intrinsically handle interaction effects either - it depends on the learning algorithm being used.  Linear models (like the Bayesian ridge regression we've been using) can't pull out interaction effects unless we explicitly encode them (by adding a column for each possible interaction).  Nonlinear learning algorithms, such as decision tree-based models, SVMs, and neural networks, are able to detect interaction effects in the data as-is.\n",
        "\n",
        "To see how well interaction effects are captured by models trained on target-encoded or one-hot-encoded data, we'll create two categorical datasets: one which has no interaction effects, and one whose variance is completely explained by interaction effects (and noise)."
      ]
    },
    {
      "metadata": {
        "id": "a-pdSDv8cvZH",
        "colab_type": "code",
        "outputId": "9ca57068-aca3-48f7-c543-e2f1167661b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "# Categorical data w/ no interaction effects\n",
        "X_no_int, y_no_int = make_categorical_regression(\n",
        "    n_samples=1000, \n",
        "    n_features=10,\n",
        "    n_categories=100,\n",
        "    n_informative=2,\n",
        "    interactions=0.0)\n",
        "\n",
        "# Categorical data w/ interaction effects\n",
        "X_inter, y_inter = make_categorical_regression(\n",
        "    n_samples=1000, \n",
        "    n_features=10,\n",
        "    n_categories=100,\n",
        "    n_informative=2,\n",
        "    interactions=1.0)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:72: DeprecationWarning: `comb` is deprecated!\n",
            "Importing `comb` from scipy.misc is deprecated in scipy 1.0.0. Use `scipy.special.comb` instead.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "pKtHi2XtauM-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To capture interaction effects, we'll have to use a model which can handle interactions, such as a tree-based method like XGBoost (a linear regression can't capture interactions unless they are explicitly encoded)."
      ]
    },
    {
      "metadata": {
        "id": "gOVZx7a6ZHr4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Regression model w/ target encoding\n",
        "model_te = Pipeline([\n",
        "    ('encoder', TargetEncoderLOO()),\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('regressor', XGBRegressor())\n",
        "])\n",
        "\n",
        "# Regression model w/ one-hot encoding\n",
        "model_oh = Pipeline([\n",
        "    ('encoder', OneHotEncoder()),\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('regressor', XGBRegressor())\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lM4UpnRYbf6k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As we've seen before, without interaction effects the target encoder performs better than the one-hot encoder."
      ]
    },
    {
      "metadata": {
        "id": "xIRPGKxrRsRd",
        "colab_type": "code",
        "outputId": "d302db4e-9d58-48fe-9d99-1576baf45cd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "print('No interaction effects:')\n",
        "\n",
        "# Target encoding w/ no interaction effects\n",
        "scores = cross_val_score(model_te, X_no_int, y_no_int, \n",
        "                         cv=5, scoring=mae_scorer)\n",
        "print('MAE w/ target encoding: %0.3f +/- %0.3f'\n",
        "      % (scores.mean(), scores.std()))\n",
        "\n",
        "# One-hot encoding w/ no interaction effects\n",
        "scores = cross_val_score(model_oh, X_no_int, y_no_int, \n",
        "                         cv=5, scoring=mae_scorer)\n",
        "print('MAE w/ one-hot encoding: %0.3f +/- %0.3f' \n",
        "      % (scores.mean(), scores.std()))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No interaction effects:\n",
            "MAE w/ target encoding: 0.981 +/- 0.058\n",
            "MAE w/ one-hot encoding: 1.119 +/- 0.065\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ty4CRP4OboGH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "However, when most of the variance can be explained by interaction effects, the model trained on one-hot encoded data performs better (or at least it's unlikely that the target-encoded model has better performance)."
      ]
    },
    {
      "metadata": {
        "id": "XxtgEG9SSBif",
        "colab_type": "code",
        "outputId": "1f54aad6-8331-4ba0-a86e-f79ee2a2a0e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "print('With interaction effects:')\n",
        "\n",
        "# Target encoding w/ interaction effects\n",
        "scores = cross_val_score(model_te, X_inter, y_inter, \n",
        "                         cv=5, scoring=mae_scorer)\n",
        "print('MAE w/ target encoding: %0.3f +/- %0.3f' \n",
        "      % (scores.mean(), scores.std()))\n",
        "\n",
        "# One-hot encoding w/ interaction effects\n",
        "scores = cross_val_score(model_oh, X_inter, y_inter, \n",
        "                         cv=5, scoring=mae_scorer)\n",
        "print('MAE w/ one-hot encoding: %0.3f +/- %0.3f' \n",
        "      % (scores.mean(), scores.std()))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "With interaction effects:\n",
            "MAE w/ target encoding: 1.240 +/- 0.052\n",
            "MAE w/ one-hot encoding: 1.184 +/- 0.058\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GgfpH7-bYWFn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Suggestions\n",
        "\n",
        "Target encoding categorical variables is a great way to represent categorical data in a numerical format that machine learning algorithms can handle, without jacking up the dimensionality of your training data.  However, make sure to use cross-fold or leave-one-out target encoding to prevent data leakage!  Also keep in mind the number of categories, what machine learning algorithm you're using, and whether you suspect there may be strong interaction effects in your data.  With only a few categories, or in the presence of interaction effects, you're probably better off just using one-hot encoding and a boosting algorithm like XGBoost/CatBoost/LightGBM.  On the other hand, if your data contains many columns with many categories, it might be best to use target encoding!"
      ]
    }
  ]
}